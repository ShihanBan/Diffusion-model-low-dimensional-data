{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e3e26c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random as random\n",
    "from datasets import LinearGaussianDataset, SphereDataset,SigmoidDataset  # ËØ∑ÊåâÂÆûÈôÖË∑ØÂæÑË∞ÉÊï¥\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bb9fd134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated intrinsic3_ambient12: X.shape = (20000, 12)\n",
      "Generated intrinsic3_ambient20: X.shape = (20000, 20)\n",
      "Generated intrinsic6_ambient12: X.shape = (20000, 12)\n",
      "Generated intrinsic6_ambient20: X.shape = (20000, 20)\n",
      "Generated intrinsic9_ambient12: X.shape = (20000, 12)\n",
      "Generated intrinsic9_ambient20: X.shape = (20000, 20)\n",
      "Generated intrinsic12_ambient20: X.shape = (20000, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_linear_gaussian(seed: int,\n",
    "                             intrinsic_dim: int,\n",
    "                             ambient_dim: int,\n",
    "                             noise_var: float = 0.0,\n",
    "                             n_samples: int = 20000):\n",
    "    \"\"\"\n",
    "    ÂØπÂ∫î run.py ‰∏≠Ôºö\n",
    "      --dataset linear_gaussian\n",
    "      -dd <intrinsic_dim>      -> dimension\n",
    "      --padding_dim <pad_dim>   -> padding_dimension\n",
    "      --var_added <noise_var>   -> var_added\n",
    "\n",
    "    ambient_dim = intrinsic_dim + padding_dim\n",
    "    \"\"\"\n",
    "    padding_dim = ambient_dim - intrinsic_dim\n",
    "    ds = LinearGaussianDataset(seed=seed,\n",
    "                                dimension=intrinsic_dim,\n",
    "                                intrinsic_dimension=intrinsic_dim,\n",
    "                                padding_dimension=padding_dim,\n",
    "                                var_added=noise_var)\n",
    "    # get_batch ËøîÂõû shape = (n_samples, intrinsic_dim + padding_dim)\n",
    "    X = ds.get_batch(n_samples)\n",
    "    return np.array(X)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ÂØπÂ∫îËÆ∫Êñá‰∏≠ËøôÂá†Êù°ÂëΩ‰ª§ÁöÑÔºàseed=2ÔºâÁ§∫‰æãÔºö\n",
    "    configs = [\n",
    "        # intrinsic=3, ambient=12\n",
    "        {\"seed\": 2,  \"intrinsic_dim\": 3,  \"ambient_dim\": 12},\n",
    "        # intrinsic=3, ambient=20\n",
    "        {\"seed\": 2,  \"intrinsic_dim\": 3,  \"ambient_dim\": 20},\n",
    "        # intrinsic=6, ambient=12\n",
    "        {\"seed\": 2,  \"intrinsic_dim\": 6,  \"ambient_dim\": 12},\n",
    "        # intrinsic=6, ambient=20\n",
    "        {\"seed\": 2,  \"intrinsic_dim\": 6,  \"ambient_dim\": 20},\n",
    "        # intrinsic=9, ambient=12\n",
    "        {\"seed\": 2,  \"intrinsic_dim\": 9,  \"ambient_dim\": 12},\n",
    "        # intrinsic=9, ambient=20\n",
    "        {\"seed\": 2,  \"intrinsic_dim\": 9,  \"ambient_dim\": 20},\n",
    "        # intrinsic=12, ambient=20\n",
    "        {\"seed\": 2,  \"intrinsic_dim\": 12, \"ambient_dim\": 20},\n",
    "    ]\n",
    "\n",
    "    all_datasets = {}\n",
    "\n",
    "    for cfg in configs:\n",
    "        key = f\"intrinsic{cfg['intrinsic_dim']}_ambient{cfg['ambient_dim']}\"\n",
    "        X = generate_linear_gaussian(seed=cfg[\"seed\"],\n",
    "                                 intrinsic_dim=cfg[\"intrinsic_dim\"],\n",
    "                                 ambient_dim=cfg[\"ambient_dim\"],\n",
    "                                 noise_var=0.0,\n",
    "                                 n_samples=20000)\n",
    "        all_datasets[key] = X\n",
    "        print(f\"Generated {key}: X.shape = {X.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e8897",
   "metadata": {},
   "source": [
    "## Diffusion Model (DDPM) - linear case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bad4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "08d74353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DenoiseMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim + 1, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = t.float().unsqueeze(-1) / 1000\n",
    "        x_input = torch.cat([x, t], dim=-1)\n",
    "        return self.net(x_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "26fdd0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(self, model, timesteps=1000, loss_type='huber'):\n",
    "        self.model = model\n",
    "        self.timesteps = timesteps\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        # Cosine beta schedule\n",
    "        steps = np.linspace(0, np.pi / 2, timesteps + 1)\n",
    "        alphas_cumprod = np.cos(steps) ** 2\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "\n",
    "        self.alphas_cumprod = torch.tensor(alphas_cumprod[:-1], dtype=torch.float32)\n",
    "        self.betas = 1 - self.alphas_cumprod[1:] / self.alphas_cumprod[:-1]\n",
    "        self.betas = torch.cat([self.betas, self.betas[-1].unsqueeze(0)], dim=0)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    def q_sample(self, x0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        sqrt_alpha_cumprod = self.alphas_cumprod[t].unsqueeze(1).sqrt().to(x0.device)\n",
    "        sqrt_one_minus = (1 - self.alphas_cumprod[t]).unsqueeze(1).sqrt().to(x0.device)\n",
    "        return sqrt_alpha_cumprod * x0 + sqrt_one_minus * noise\n",
    "\n",
    "    def p_losses(self, x0, t):\n",
    "        noise = torch.randn_like(x0)\n",
    "        x_noisy = self.q_sample(x0, t, noise)\n",
    "        noise_pred = self.model(x_noisy, t)\n",
    "        if self.loss_type == 'mse':\n",
    "            return F.mse_loss(noise_pred, noise)\n",
    "        elif self.loss_type == 'huber':\n",
    "            return F.smooth_l1_loss(noise_pred, noise)\n",
    "\n",
    "    def p_sample(self, x, t):\n",
    "        noise_pred = self.model(x, t)\n",
    "        beta_t = self.betas[t].unsqueeze(1).to(x.device)\n",
    "        alpha_t = self.alphas[t].unsqueeze(1).to(x.device)\n",
    "        alpha_cumprod_t = self.alphas_cumprod[t].unsqueeze(1).to(x.device)\n",
    "\n",
    "        mean = (1 / alpha_t.sqrt()) * (x - beta_t / (1 - alpha_cumprod_t).sqrt() * noise_pred)\n",
    "\n",
    "        if t[0] == 0:\n",
    "            return mean\n",
    "        else:\n",
    "            noise = torch.randn_like(x)\n",
    "            return mean + beta_t.sqrt() * noise\n",
    "\n",
    "    def sample(self, num_samples, dim, device):\n",
    "        x = torch.randn(num_samples, dim).to(device)\n",
    "        for t in reversed(range(self.timesteps)):\n",
    "            t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
    "            x = self.p_sample(x, t_batch)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d59dedc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffusion_and_evaluate(all_datasets, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from sklearn.decomposition import PCA\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    os.makedirs(\"generated_samples\", exist_ok=True)\n",
    "    final_results = {}\n",
    "\n",
    "    def get_intrinsic_dim_from_key(key):\n",
    "        try:\n",
    "            return int(key.split('_')[0].replace('intrinsic', ''))\n",
    "        except:\n",
    "            raise ValueError(f\"Cannot parse intrinsic dimension from key: {key}\")\n",
    "\n",
    "    def mean_normalized_eigenvalue_error(X_real, X_fake, intrinsic_dim):\n",
    "        pca_real = PCA(n_components=intrinsic_dim).fit(X_real)\n",
    "        pca_fake = PCA(n_components=intrinsic_dim).fit(X_fake)\n",
    "        Œª_real = pca_real.explained_variance_\n",
    "        Œª_fake = pca_fake.explained_variance_\n",
    "        return np.linalg.norm(Œª_fake - Œª_real) / np.linalg.norm(Œª_real)\n",
    "\n",
    "    def explained_variance_coverage_error(X_real, X_fake, intrinsic_dim):\n",
    "        pca_real = PCA().fit(X_real)\n",
    "        pca_fake = PCA().fit(X_fake)\n",
    "        var_real = pca_real.explained_variance_ratio_\n",
    "        var_fake = pca_fake.explained_variance_ratio_\n",
    "        return abs(np.sum(var_fake[:intrinsic_dim]) - np.sum(var_real[:intrinsic_dim]))\n",
    "\n",
    "    for key, X in all_datasets.items():\n",
    "        print(f\"\\nüöÄ Training on config: {key}, shape = {X.shape}\")\n",
    "        intrinsic_dim = get_intrinsic_dim_from_key(key)\n",
    "\n",
    "        evce_runs = []\n",
    "        mnee_runs = []\n",
    "\n",
    "        for run in range(3):  # ‰∏âÊ¨°ÂÆûÈ™å\n",
    "            print(f\"üîÅ Run {run+1}/3\")\n",
    "\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            dataset = TensorDataset(X_tensor)\n",
    "            dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "            model = DenoiseMLP(input_dim=X.shape[1]).to(device)\n",
    "            diffusion = Diffusion(model, timesteps=1000, loss_type='huber')\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "            # === Train ===\n",
    "            for epoch in range(500):\n",
    "                losses = []\n",
    "                for batch in dataloader:\n",
    "                    x0 = batch[0].to(device)\n",
    "                    t = torch.randint(0, diffusion.timesteps, (x0.shape[0],), device=device)\n",
    "                    loss = diffusion.p_losses(x0, t)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                    avg_loss = sum(losses) / len(losses)\n",
    "                    print(f\"[{key}] Epoch {epoch+1} - Avg Loss: {avg_loss:.6f}\")\n",
    "\n",
    "\n",
    "            # === Sample and Evaluate ===\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                samples = diffusion.sample(num_samples=20000, dim=X.shape[1], device=device)\n",
    "                samples = samples.cpu().numpy()\n",
    "                np.save(f\"generated_samples/{key}_run{run+1}_samples.npy\", samples)\n",
    "\n",
    "            evce = explained_variance_coverage_error(X, samples, intrinsic_dim)\n",
    "            mnee = mean_normalized_eigenvalue_error(X, samples, intrinsic_dim)\n",
    "            evce_runs.append(evce)\n",
    "            mnee_runs.append(mnee)\n",
    "\n",
    "        evce_mean = np.mean(evce_runs)\n",
    "        mnee_mean = np.mean(mnee_runs)\n",
    "        final_results[key] = {\"EVCE\": evce_mean, \"MNEE\": mnee_mean}\n",
    "\n",
    "        print(f\"[{key}] üîç EVCE (dim alignment): {evce_mean:.6f}\")\n",
    "        print(f\"[{key}] üìâ MNEE (density error):  {mnee_mean:.6f}\")\n",
    "\n",
    "    print(\"\\nüìä Final Aggregated Results:\")\n",
    "    for key, vals in final_results.items():\n",
    "        print(f\"{key} ‚Üí EVCE: {vals['EVCE']:.4f}, MNEE: {vals['MNEE']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3df3942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training on config: intrinsic3_ambient12, shape = (20000, 12)\n",
      "üîÅ Run 1/3\n",
      "[intrinsic3_ambient12] Epoch 1 - Avg Loss: 0.332755\n",
      "[intrinsic3_ambient12] Epoch 10 - Avg Loss: 0.116237\n",
      "[intrinsic3_ambient12] Epoch 20 - Avg Loss: 0.093720\n",
      "[intrinsic3_ambient12] Epoch 30 - Avg Loss: 0.085155\n",
      "[intrinsic3_ambient12] Epoch 40 - Avg Loss: 0.079815\n",
      "[intrinsic3_ambient12] Epoch 50 - Avg Loss: 0.076738\n",
      "[intrinsic3_ambient12] Epoch 60 - Avg Loss: 0.075812\n",
      "[intrinsic3_ambient12] Epoch 70 - Avg Loss: 0.073393\n",
      "[intrinsic3_ambient12] Epoch 80 - Avg Loss: 0.071487\n",
      "[intrinsic3_ambient12] Epoch 90 - Avg Loss: 0.071045\n",
      "[intrinsic3_ambient12] Epoch 100 - Avg Loss: 0.070905\n",
      "[intrinsic3_ambient12] Epoch 110 - Avg Loss: 0.069909\n",
      "[intrinsic3_ambient12] Epoch 120 - Avg Loss: 0.070499\n",
      "[intrinsic3_ambient12] Epoch 130 - Avg Loss: 0.068606\n",
      "[intrinsic3_ambient12] Epoch 140 - Avg Loss: 0.068266\n",
      "[intrinsic3_ambient12] Epoch 150 - Avg Loss: 0.069321\n",
      "[intrinsic3_ambient12] Epoch 160 - Avg Loss: 0.068967\n",
      "[intrinsic3_ambient12] Epoch 170 - Avg Loss: 0.067982\n",
      "[intrinsic3_ambient12] Epoch 180 - Avg Loss: 0.068951\n",
      "[intrinsic3_ambient12] Epoch 190 - Avg Loss: 0.068296\n",
      "[intrinsic3_ambient12] Epoch 200 - Avg Loss: 0.067264\n",
      "[intrinsic3_ambient12] Epoch 210 - Avg Loss: 0.068514\n",
      "[intrinsic3_ambient12] Epoch 220 - Avg Loss: 0.068101\n",
      "[intrinsic3_ambient12] Epoch 230 - Avg Loss: 0.068590\n",
      "[intrinsic3_ambient12] Epoch 240 - Avg Loss: 0.066931\n",
      "[intrinsic3_ambient12] Epoch 250 - Avg Loss: 0.067399\n",
      "[intrinsic3_ambient12] Epoch 260 - Avg Loss: 0.067378\n",
      "[intrinsic3_ambient12] Epoch 270 - Avg Loss: 0.066888\n",
      "[intrinsic3_ambient12] Epoch 280 - Avg Loss: 0.066671\n",
      "[intrinsic3_ambient12] Epoch 290 - Avg Loss: 0.068024\n",
      "[intrinsic3_ambient12] Epoch 300 - Avg Loss: 0.067105\n",
      "[intrinsic3_ambient12] Epoch 310 - Avg Loss: 0.067151\n",
      "[intrinsic3_ambient12] Epoch 320 - Avg Loss: 0.066009\n",
      "[intrinsic3_ambient12] Epoch 330 - Avg Loss: 0.066528\n",
      "[intrinsic3_ambient12] Epoch 340 - Avg Loss: 0.066834\n",
      "[intrinsic3_ambient12] Epoch 350 - Avg Loss: 0.065849\n",
      "[intrinsic3_ambient12] Epoch 360 - Avg Loss: 0.066498\n",
      "[intrinsic3_ambient12] Epoch 370 - Avg Loss: 0.066658\n",
      "[intrinsic3_ambient12] Epoch 380 - Avg Loss: 0.066386\n",
      "[intrinsic3_ambient12] Epoch 390 - Avg Loss: 0.065998\n",
      "[intrinsic3_ambient12] Epoch 400 - Avg Loss: 0.065706\n",
      "[intrinsic3_ambient12] Epoch 410 - Avg Loss: 0.066082\n",
      "[intrinsic3_ambient12] Epoch 420 - Avg Loss: 0.065453\n",
      "[intrinsic3_ambient12] Epoch 430 - Avg Loss: 0.066092\n",
      "[intrinsic3_ambient12] Epoch 440 - Avg Loss: 0.065553\n",
      "[intrinsic3_ambient12] Epoch 450 - Avg Loss: 0.065505\n",
      "[intrinsic3_ambient12] Epoch 460 - Avg Loss: 0.066029\n",
      "[intrinsic3_ambient12] Epoch 470 - Avg Loss: 0.065334\n",
      "[intrinsic3_ambient12] Epoch 480 - Avg Loss: 0.065227\n",
      "[intrinsic3_ambient12] Epoch 490 - Avg Loss: 0.065863\n",
      "[intrinsic3_ambient12] Epoch 500 - Avg Loss: 0.065018\n",
      "üîÅ Run 2/3\n",
      "[intrinsic3_ambient12] Epoch 1 - Avg Loss: 0.341735\n",
      "[intrinsic3_ambient12] Epoch 10 - Avg Loss: 0.117059\n",
      "[intrinsic3_ambient12] Epoch 20 - Avg Loss: 0.093792\n",
      "[intrinsic3_ambient12] Epoch 30 - Avg Loss: 0.086308\n",
      "[intrinsic3_ambient12] Epoch 40 - Avg Loss: 0.080204\n",
      "[intrinsic3_ambient12] Epoch 50 - Avg Loss: 0.078020\n",
      "[intrinsic3_ambient12] Epoch 60 - Avg Loss: 0.075682\n",
      "[intrinsic3_ambient12] Epoch 70 - Avg Loss: 0.074561\n",
      "[intrinsic3_ambient12] Epoch 80 - Avg Loss: 0.073153\n",
      "[intrinsic3_ambient12] Epoch 90 - Avg Loss: 0.072081\n",
      "[intrinsic3_ambient12] Epoch 100 - Avg Loss: 0.070226\n",
      "[intrinsic3_ambient12] Epoch 110 - Avg Loss: 0.070817\n",
      "[intrinsic3_ambient12] Epoch 120 - Avg Loss: 0.069562\n",
      "[intrinsic3_ambient12] Epoch 130 - Avg Loss: 0.069801\n",
      "[intrinsic3_ambient12] Epoch 140 - Avg Loss: 0.068476\n",
      "[intrinsic3_ambient12] Epoch 150 - Avg Loss: 0.068735\n",
      "[intrinsic3_ambient12] Epoch 160 - Avg Loss: 0.068952\n",
      "[intrinsic3_ambient12] Epoch 170 - Avg Loss: 0.069372\n",
      "[intrinsic3_ambient12] Epoch 180 - Avg Loss: 0.068735\n",
      "[intrinsic3_ambient12] Epoch 190 - Avg Loss: 0.067804\n",
      "[intrinsic3_ambient12] Epoch 200 - Avg Loss: 0.067901\n",
      "[intrinsic3_ambient12] Epoch 210 - Avg Loss: 0.067725\n",
      "[intrinsic3_ambient12] Epoch 220 - Avg Loss: 0.066988\n",
      "[intrinsic3_ambient12] Epoch 230 - Avg Loss: 0.067539\n",
      "[intrinsic3_ambient12] Epoch 240 - Avg Loss: 0.067128\n",
      "[intrinsic3_ambient12] Epoch 250 - Avg Loss: 0.067347\n",
      "[intrinsic3_ambient12] Epoch 260 - Avg Loss: 0.066429\n",
      "[intrinsic3_ambient12] Epoch 270 - Avg Loss: 0.067288\n",
      "[intrinsic3_ambient12] Epoch 280 - Avg Loss: 0.066666\n",
      "[intrinsic3_ambient12] Epoch 290 - Avg Loss: 0.066879\n",
      "[intrinsic3_ambient12] Epoch 300 - Avg Loss: 0.066318\n",
      "[intrinsic3_ambient12] Epoch 310 - Avg Loss: 0.066082\n",
      "[intrinsic3_ambient12] Epoch 320 - Avg Loss: 0.066604\n",
      "[intrinsic3_ambient12] Epoch 330 - Avg Loss: 0.066356\n",
      "[intrinsic3_ambient12] Epoch 340 - Avg Loss: 0.066780\n",
      "[intrinsic3_ambient12] Epoch 350 - Avg Loss: 0.066745\n",
      "[intrinsic3_ambient12] Epoch 360 - Avg Loss: 0.066584\n",
      "[intrinsic3_ambient12] Epoch 370 - Avg Loss: 0.066364\n",
      "[intrinsic3_ambient12] Epoch 380 - Avg Loss: 0.066881\n",
      "[intrinsic3_ambient12] Epoch 390 - Avg Loss: 0.066703\n",
      "[intrinsic3_ambient12] Epoch 400 - Avg Loss: 0.065606\n",
      "[intrinsic3_ambient12] Epoch 410 - Avg Loss: 0.065993\n",
      "[intrinsic3_ambient12] Epoch 420 - Avg Loss: 0.065805\n",
      "[intrinsic3_ambient12] Epoch 430 - Avg Loss: 0.065975\n",
      "[intrinsic3_ambient12] Epoch 440 - Avg Loss: 0.065816\n",
      "[intrinsic3_ambient12] Epoch 450 - Avg Loss: 0.066626\n",
      "[intrinsic3_ambient12] Epoch 460 - Avg Loss: 0.065945\n",
      "[intrinsic3_ambient12] Epoch 470 - Avg Loss: 0.066334\n",
      "[intrinsic3_ambient12] Epoch 480 - Avg Loss: 0.065855\n",
      "[intrinsic3_ambient12] Epoch 490 - Avg Loss: 0.065555\n",
      "[intrinsic3_ambient12] Epoch 500 - Avg Loss: 0.065126\n",
      "üîÅ Run 3/3\n",
      "[intrinsic3_ambient12] Epoch 1 - Avg Loss: 0.333892\n",
      "[intrinsic3_ambient12] Epoch 10 - Avg Loss: 0.114516\n",
      "[intrinsic3_ambient12] Epoch 20 - Avg Loss: 0.093148\n",
      "[intrinsic3_ambient12] Epoch 30 - Avg Loss: 0.084177\n",
      "[intrinsic3_ambient12] Epoch 40 - Avg Loss: 0.080533\n",
      "[intrinsic3_ambient12] Epoch 50 - Avg Loss: 0.077046\n",
      "[intrinsic3_ambient12] Epoch 60 - Avg Loss: 0.074837\n",
      "[intrinsic3_ambient12] Epoch 70 - Avg Loss: 0.073145\n",
      "[intrinsic3_ambient12] Epoch 80 - Avg Loss: 0.071379\n",
      "[intrinsic3_ambient12] Epoch 90 - Avg Loss: 0.070596\n",
      "[intrinsic3_ambient12] Epoch 100 - Avg Loss: 0.071090\n",
      "[intrinsic3_ambient12] Epoch 110 - Avg Loss: 0.070218\n",
      "[intrinsic3_ambient12] Epoch 120 - Avg Loss: 0.069914\n",
      "[intrinsic3_ambient12] Epoch 130 - Avg Loss: 0.069316\n",
      "[intrinsic3_ambient12] Epoch 140 - Avg Loss: 0.069525\n",
      "[intrinsic3_ambient12] Epoch 150 - Avg Loss: 0.068884\n",
      "[intrinsic3_ambient12] Epoch 160 - Avg Loss: 0.068298\n",
      "[intrinsic3_ambient12] Epoch 170 - Avg Loss: 0.068214\n",
      "[intrinsic3_ambient12] Epoch 180 - Avg Loss: 0.068338\n",
      "[intrinsic3_ambient12] Epoch 190 - Avg Loss: 0.068134\n",
      "[intrinsic3_ambient12] Epoch 200 - Avg Loss: 0.067534\n",
      "[intrinsic3_ambient12] Epoch 210 - Avg Loss: 0.067311\n",
      "[intrinsic3_ambient12] Epoch 220 - Avg Loss: 0.068349\n",
      "[intrinsic3_ambient12] Epoch 230 - Avg Loss: 0.067031\n",
      "[intrinsic3_ambient12] Epoch 240 - Avg Loss: 0.066919\n",
      "[intrinsic3_ambient12] Epoch 250 - Avg Loss: 0.067346\n",
      "[intrinsic3_ambient12] Epoch 260 - Avg Loss: 0.067612\n",
      "[intrinsic3_ambient12] Epoch 270 - Avg Loss: 0.067504\n",
      "[intrinsic3_ambient12] Epoch 280 - Avg Loss: 0.067908\n",
      "[intrinsic3_ambient12] Epoch 290 - Avg Loss: 0.067174\n",
      "[intrinsic3_ambient12] Epoch 300 - Avg Loss: 0.066483\n",
      "[intrinsic3_ambient12] Epoch 310 - Avg Loss: 0.066853\n",
      "[intrinsic3_ambient12] Epoch 320 - Avg Loss: 0.066982\n",
      "[intrinsic3_ambient12] Epoch 330 - Avg Loss: 0.066122\n",
      "[intrinsic3_ambient12] Epoch 340 - Avg Loss: 0.066744\n",
      "[intrinsic3_ambient12] Epoch 350 - Avg Loss: 0.066245\n",
      "[intrinsic3_ambient12] Epoch 360 - Avg Loss: 0.065741\n",
      "[intrinsic3_ambient12] Epoch 370 - Avg Loss: 0.065774\n",
      "[intrinsic3_ambient12] Epoch 380 - Avg Loss: 0.066447\n",
      "[intrinsic3_ambient12] Epoch 390 - Avg Loss: 0.066905\n",
      "[intrinsic3_ambient12] Epoch 400 - Avg Loss: 0.066352\n",
      "[intrinsic3_ambient12] Epoch 410 - Avg Loss: 0.065986\n",
      "[intrinsic3_ambient12] Epoch 420 - Avg Loss: 0.065976\n",
      "[intrinsic3_ambient12] Epoch 430 - Avg Loss: 0.067251\n",
      "[intrinsic3_ambient12] Epoch 440 - Avg Loss: 0.065935\n",
      "[intrinsic3_ambient12] Epoch 450 - Avg Loss: 0.065705\n",
      "[intrinsic3_ambient12] Epoch 460 - Avg Loss: 0.065698\n",
      "[intrinsic3_ambient12] Epoch 470 - Avg Loss: 0.066081\n",
      "[intrinsic3_ambient12] Epoch 480 - Avg Loss: 0.065776\n",
      "[intrinsic3_ambient12] Epoch 490 - Avg Loss: 0.065230\n",
      "[intrinsic3_ambient12] Epoch 500 - Avg Loss: 0.065816\n",
      "[intrinsic3_ambient12] üîç EVCE (dim alignment): 0.000022\n",
      "[intrinsic3_ambient12] üìâ MNEE (density error):  0.083067\n",
      "\n",
      "üöÄ Training on config: intrinsic3_ambient20, shape = (20000, 20)\n",
      "üîÅ Run 1/3\n",
      "[intrinsic3_ambient20] Epoch 1 - Avg Loss: 0.371395\n",
      "[intrinsic3_ambient20] Epoch 10 - Avg Loss: 0.103728\n",
      "[intrinsic3_ambient20] Epoch 20 - Avg Loss: 0.076683\n",
      "[intrinsic3_ambient20] Epoch 30 - Avg Loss: 0.066340\n",
      "[intrinsic3_ambient20] Epoch 40 - Avg Loss: 0.061348\n",
      "[intrinsic3_ambient20] Epoch 50 - Avg Loss: 0.057410\n",
      "[intrinsic3_ambient20] Epoch 60 - Avg Loss: 0.055209\n",
      "[intrinsic3_ambient20] Epoch 70 - Avg Loss: 0.053174\n",
      "[intrinsic3_ambient20] Epoch 80 - Avg Loss: 0.051731\n",
      "[intrinsic3_ambient20] Epoch 90 - Avg Loss: 0.050651\n",
      "[intrinsic3_ambient20] Epoch 100 - Avg Loss: 0.049040\n",
      "[intrinsic3_ambient20] Epoch 110 - Avg Loss: 0.049339\n",
      "[intrinsic3_ambient20] Epoch 120 - Avg Loss: 0.048489\n",
      "[intrinsic3_ambient20] Epoch 130 - Avg Loss: 0.047918\n",
      "[intrinsic3_ambient20] Epoch 140 - Avg Loss: 0.047132\n",
      "[intrinsic3_ambient20] Epoch 150 - Avg Loss: 0.047109\n",
      "[intrinsic3_ambient20] Epoch 160 - Avg Loss: 0.046484\n",
      "[intrinsic3_ambient20] Epoch 170 - Avg Loss: 0.046582\n",
      "[intrinsic3_ambient20] Epoch 180 - Avg Loss: 0.045100\n",
      "[intrinsic3_ambient20] Epoch 190 - Avg Loss: 0.045793\n",
      "[intrinsic3_ambient20] Epoch 200 - Avg Loss: 0.045059\n",
      "[intrinsic3_ambient20] Epoch 210 - Avg Loss: 0.045216\n",
      "[intrinsic3_ambient20] Epoch 220 - Avg Loss: 0.044692\n",
      "[intrinsic3_ambient20] Epoch 230 - Avg Loss: 0.045272\n",
      "[intrinsic3_ambient20] Epoch 240 - Avg Loss: 0.044407\n",
      "[intrinsic3_ambient20] Epoch 250 - Avg Loss: 0.044460\n",
      "[intrinsic3_ambient20] Epoch 260 - Avg Loss: 0.044597\n",
      "[intrinsic3_ambient20] Epoch 270 - Avg Loss: 0.043944\n",
      "[intrinsic3_ambient20] Epoch 280 - Avg Loss: 0.043997\n",
      "[intrinsic3_ambient20] Epoch 290 - Avg Loss: 0.043847\n",
      "[intrinsic3_ambient20] Epoch 300 - Avg Loss: 0.043665\n",
      "[intrinsic3_ambient20] Epoch 310 - Avg Loss: 0.043120\n",
      "[intrinsic3_ambient20] Epoch 320 - Avg Loss: 0.043264\n",
      "[intrinsic3_ambient20] Epoch 330 - Avg Loss: 0.043121\n",
      "[intrinsic3_ambient20] Epoch 340 - Avg Loss: 0.043079\n",
      "[intrinsic3_ambient20] Epoch 350 - Avg Loss: 0.043787\n",
      "[intrinsic3_ambient20] Epoch 360 - Avg Loss: 0.042965\n",
      "[intrinsic3_ambient20] Epoch 370 - Avg Loss: 0.043193\n",
      "[intrinsic3_ambient20] Epoch 380 - Avg Loss: 0.043173\n",
      "[intrinsic3_ambient20] Epoch 390 - Avg Loss: 0.043115\n",
      "[intrinsic3_ambient20] Epoch 400 - Avg Loss: 0.042910\n",
      "[intrinsic3_ambient20] Epoch 410 - Avg Loss: 0.042427\n",
      "[intrinsic3_ambient20] Epoch 420 - Avg Loss: 0.043147\n",
      "[intrinsic3_ambient20] Epoch 430 - Avg Loss: 0.042776\n",
      "[intrinsic3_ambient20] Epoch 440 - Avg Loss: 0.042449\n",
      "[intrinsic3_ambient20] Epoch 450 - Avg Loss: 0.042603\n",
      "[intrinsic3_ambient20] Epoch 460 - Avg Loss: 0.042604\n",
      "[intrinsic3_ambient20] Epoch 470 - Avg Loss: 0.042491\n",
      "[intrinsic3_ambient20] Epoch 480 - Avg Loss: 0.042524\n",
      "[intrinsic3_ambient20] Epoch 490 - Avg Loss: 0.041806\n",
      "[intrinsic3_ambient20] Epoch 500 - Avg Loss: 0.042105\n",
      "üîÅ Run 2/3\n",
      "[intrinsic3_ambient20] Epoch 1 - Avg Loss: 0.367568\n",
      "[intrinsic3_ambient20] Epoch 10 - Avg Loss: 0.101582\n",
      "[intrinsic3_ambient20] Epoch 20 - Avg Loss: 0.077061\n",
      "[intrinsic3_ambient20] Epoch 30 - Avg Loss: 0.066268\n",
      "[intrinsic3_ambient20] Epoch 40 - Avg Loss: 0.060938\n",
      "[intrinsic3_ambient20] Epoch 50 - Avg Loss: 0.056745\n",
      "[intrinsic3_ambient20] Epoch 60 - Avg Loss: 0.054553\n",
      "[intrinsic3_ambient20] Epoch 70 - Avg Loss: 0.053470\n",
      "[intrinsic3_ambient20] Epoch 80 - Avg Loss: 0.051215\n",
      "[intrinsic3_ambient20] Epoch 90 - Avg Loss: 0.050028\n",
      "[intrinsic3_ambient20] Epoch 100 - Avg Loss: 0.050213\n",
      "[intrinsic3_ambient20] Epoch 110 - Avg Loss: 0.049393\n",
      "[intrinsic3_ambient20] Epoch 120 - Avg Loss: 0.048203\n",
      "[intrinsic3_ambient20] Epoch 130 - Avg Loss: 0.047976\n",
      "[intrinsic3_ambient20] Epoch 140 - Avg Loss: 0.047216\n",
      "[intrinsic3_ambient20] Epoch 150 - Avg Loss: 0.046749\n",
      "[intrinsic3_ambient20] Epoch 160 - Avg Loss: 0.046386\n",
      "[intrinsic3_ambient20] Epoch 170 - Avg Loss: 0.045863\n",
      "[intrinsic3_ambient20] Epoch 180 - Avg Loss: 0.045302\n",
      "[intrinsic3_ambient20] Epoch 190 - Avg Loss: 0.045151\n",
      "[intrinsic3_ambient20] Epoch 200 - Avg Loss: 0.045478\n",
      "[intrinsic3_ambient20] Epoch 210 - Avg Loss: 0.045456\n",
      "[intrinsic3_ambient20] Epoch 220 - Avg Loss: 0.045745\n",
      "[intrinsic3_ambient20] Epoch 230 - Avg Loss: 0.044916\n",
      "[intrinsic3_ambient20] Epoch 240 - Avg Loss: 0.044606\n",
      "[intrinsic3_ambient20] Epoch 250 - Avg Loss: 0.044368\n",
      "[intrinsic3_ambient20] Epoch 260 - Avg Loss: 0.044176\n",
      "[intrinsic3_ambient20] Epoch 270 - Avg Loss: 0.044121\n",
      "[intrinsic3_ambient20] Epoch 280 - Avg Loss: 0.044066\n",
      "[intrinsic3_ambient20] Epoch 290 - Avg Loss: 0.044264\n",
      "[intrinsic3_ambient20] Epoch 300 - Avg Loss: 0.044216\n",
      "[intrinsic3_ambient20] Epoch 310 - Avg Loss: 0.043761\n",
      "[intrinsic3_ambient20] Epoch 320 - Avg Loss: 0.043324\n",
      "[intrinsic3_ambient20] Epoch 330 - Avg Loss: 0.043400\n",
      "[intrinsic3_ambient20] Epoch 340 - Avg Loss: 0.043504\n",
      "[intrinsic3_ambient20] Epoch 350 - Avg Loss: 0.043288\n",
      "[intrinsic3_ambient20] Epoch 360 - Avg Loss: 0.043487\n",
      "[intrinsic3_ambient20] Epoch 370 - Avg Loss: 0.043275\n",
      "[intrinsic3_ambient20] Epoch 380 - Avg Loss: 0.042626\n",
      "[intrinsic3_ambient20] Epoch 390 - Avg Loss: 0.042659\n",
      "[intrinsic3_ambient20] Epoch 400 - Avg Loss: 0.042690\n",
      "[intrinsic3_ambient20] Epoch 410 - Avg Loss: 0.042954\n",
      "[intrinsic3_ambient20] Epoch 420 - Avg Loss: 0.042798\n",
      "[intrinsic3_ambient20] Epoch 430 - Avg Loss: 0.042020\n",
      "[intrinsic3_ambient20] Epoch 440 - Avg Loss: 0.042790\n",
      "[intrinsic3_ambient20] Epoch 450 - Avg Loss: 0.043028\n",
      "[intrinsic3_ambient20] Epoch 460 - Avg Loss: 0.042719\n",
      "[intrinsic3_ambient20] Epoch 470 - Avg Loss: 0.042553\n",
      "[intrinsic3_ambient20] Epoch 480 - Avg Loss: 0.042679\n",
      "[intrinsic3_ambient20] Epoch 490 - Avg Loss: 0.042015\n",
      "[intrinsic3_ambient20] Epoch 500 - Avg Loss: 0.042054\n",
      "üîÅ Run 3/3\n",
      "[intrinsic3_ambient20] Epoch 1 - Avg Loss: 0.364485\n",
      "[intrinsic3_ambient20] Epoch 10 - Avg Loss: 0.104048\n",
      "[intrinsic3_ambient20] Epoch 20 - Avg Loss: 0.078572\n",
      "[intrinsic3_ambient20] Epoch 30 - Avg Loss: 0.065695\n",
      "[intrinsic3_ambient20] Epoch 40 - Avg Loss: 0.060871\n",
      "[intrinsic3_ambient20] Epoch 50 - Avg Loss: 0.058224\n",
      "[intrinsic3_ambient20] Epoch 60 - Avg Loss: 0.054924\n",
      "[intrinsic3_ambient20] Epoch 70 - Avg Loss: 0.053544\n",
      "[intrinsic3_ambient20] Epoch 80 - Avg Loss: 0.052212\n",
      "[intrinsic3_ambient20] Epoch 90 - Avg Loss: 0.050295\n",
      "[intrinsic3_ambient20] Epoch 100 - Avg Loss: 0.049812\n",
      "[intrinsic3_ambient20] Epoch 110 - Avg Loss: 0.048765\n",
      "[intrinsic3_ambient20] Epoch 120 - Avg Loss: 0.048356\n",
      "[intrinsic3_ambient20] Epoch 130 - Avg Loss: 0.047293\n",
      "[intrinsic3_ambient20] Epoch 140 - Avg Loss: 0.046916\n",
      "[intrinsic3_ambient20] Epoch 150 - Avg Loss: 0.047673\n",
      "[intrinsic3_ambient20] Epoch 160 - Avg Loss: 0.045710\n",
      "[intrinsic3_ambient20] Epoch 170 - Avg Loss: 0.045484\n",
      "[intrinsic3_ambient20] Epoch 180 - Avg Loss: 0.045933\n",
      "[intrinsic3_ambient20] Epoch 190 - Avg Loss: 0.045259\n",
      "[intrinsic3_ambient20] Epoch 200 - Avg Loss: 0.045479\n",
      "[intrinsic3_ambient20] Epoch 210 - Avg Loss: 0.044763\n",
      "[intrinsic3_ambient20] Epoch 220 - Avg Loss: 0.044914\n",
      "[intrinsic3_ambient20] Epoch 230 - Avg Loss: 0.045353\n",
      "[intrinsic3_ambient20] Epoch 240 - Avg Loss: 0.044378\n",
      "[intrinsic3_ambient20] Epoch 250 - Avg Loss: 0.044720\n",
      "[intrinsic3_ambient20] Epoch 260 - Avg Loss: 0.043752\n",
      "[intrinsic3_ambient20] Epoch 270 - Avg Loss: 0.044089\n",
      "[intrinsic3_ambient20] Epoch 280 - Avg Loss: 0.043996\n",
      "[intrinsic3_ambient20] Epoch 290 - Avg Loss: 0.044196\n",
      "[intrinsic3_ambient20] Epoch 300 - Avg Loss: 0.043817\n",
      "[intrinsic3_ambient20] Epoch 310 - Avg Loss: 0.043778\n",
      "[intrinsic3_ambient20] Epoch 320 - Avg Loss: 0.043693\n",
      "[intrinsic3_ambient20] Epoch 330 - Avg Loss: 0.043226\n",
      "[intrinsic3_ambient20] Epoch 340 - Avg Loss: 0.043068\n",
      "[intrinsic3_ambient20] Epoch 350 - Avg Loss: 0.043200\n",
      "[intrinsic3_ambient20] Epoch 360 - Avg Loss: 0.043658\n",
      "[intrinsic3_ambient20] Epoch 370 - Avg Loss: 0.043463\n",
      "[intrinsic3_ambient20] Epoch 380 - Avg Loss: 0.042409\n",
      "[intrinsic3_ambient20] Epoch 390 - Avg Loss: 0.043025\n",
      "[intrinsic3_ambient20] Epoch 400 - Avg Loss: 0.043238\n",
      "[intrinsic3_ambient20] Epoch 410 - Avg Loss: 0.042775\n",
      "[intrinsic3_ambient20] Epoch 420 - Avg Loss: 0.042649\n",
      "[intrinsic3_ambient20] Epoch 430 - Avg Loss: 0.043095\n",
      "[intrinsic3_ambient20] Epoch 440 - Avg Loss: 0.043031\n",
      "[intrinsic3_ambient20] Epoch 450 - Avg Loss: 0.042730\n",
      "[intrinsic3_ambient20] Epoch 460 - Avg Loss: 0.042929\n",
      "[intrinsic3_ambient20] Epoch 470 - Avg Loss: 0.042329\n",
      "[intrinsic3_ambient20] Epoch 480 - Avg Loss: 0.042627\n",
      "[intrinsic3_ambient20] Epoch 490 - Avg Loss: 0.042430\n",
      "[intrinsic3_ambient20] Epoch 500 - Avg Loss: 0.042038\n",
      "[intrinsic3_ambient20] üîç EVCE (dim alignment): 0.000066\n",
      "[intrinsic3_ambient20] üìâ MNEE (density error):  0.213562\n",
      "\n",
      "üöÄ Training on config: intrinsic6_ambient12, shape = (20000, 12)\n",
      "üîÅ Run 1/3\n",
      "[intrinsic6_ambient12] Epoch 1 - Avg Loss: 0.351291\n",
      "[intrinsic6_ambient12] Epoch 10 - Avg Loss: 0.170159\n",
      "[intrinsic6_ambient12] Epoch 20 - Avg Loss: 0.153973\n",
      "[intrinsic6_ambient12] Epoch 30 - Avg Loss: 0.143062\n",
      "[intrinsic6_ambient12] Epoch 40 - Avg Loss: 0.138276\n",
      "[intrinsic6_ambient12] Epoch 50 - Avg Loss: 0.133840\n",
      "[intrinsic6_ambient12] Epoch 60 - Avg Loss: 0.129092\n",
      "[intrinsic6_ambient12] Epoch 70 - Avg Loss: 0.128363\n",
      "[intrinsic6_ambient12] Epoch 80 - Avg Loss: 0.128861\n",
      "[intrinsic6_ambient12] Epoch 90 - Avg Loss: 0.127883\n",
      "[intrinsic6_ambient12] Epoch 100 - Avg Loss: 0.126961\n",
      "[intrinsic6_ambient12] Epoch 110 - Avg Loss: 0.124867\n",
      "[intrinsic6_ambient12] Epoch 120 - Avg Loss: 0.124860\n",
      "[intrinsic6_ambient12] Epoch 130 - Avg Loss: 0.124155\n",
      "[intrinsic6_ambient12] Epoch 140 - Avg Loss: 0.124899\n",
      "[intrinsic6_ambient12] Epoch 150 - Avg Loss: 0.124153\n",
      "[intrinsic6_ambient12] Epoch 160 - Avg Loss: 0.124272\n",
      "[intrinsic6_ambient12] Epoch 170 - Avg Loss: 0.122896\n",
      "[intrinsic6_ambient12] Epoch 180 - Avg Loss: 0.123235\n",
      "[intrinsic6_ambient12] Epoch 190 - Avg Loss: 0.123675\n",
      "[intrinsic6_ambient12] Epoch 200 - Avg Loss: 0.123493\n",
      "[intrinsic6_ambient12] Epoch 210 - Avg Loss: 0.123064\n",
      "[intrinsic6_ambient12] Epoch 220 - Avg Loss: 0.121778\n",
      "[intrinsic6_ambient12] Epoch 230 - Avg Loss: 0.122147\n",
      "[intrinsic6_ambient12] Epoch 240 - Avg Loss: 0.122041\n",
      "[intrinsic6_ambient12] Epoch 250 - Avg Loss: 0.121357\n",
      "[intrinsic6_ambient12] Epoch 260 - Avg Loss: 0.121042\n",
      "[intrinsic6_ambient12] Epoch 270 - Avg Loss: 0.123259\n",
      "[intrinsic6_ambient12] Epoch 280 - Avg Loss: 0.121005\n",
      "[intrinsic6_ambient12] Epoch 290 - Avg Loss: 0.121810\n",
      "[intrinsic6_ambient12] Epoch 300 - Avg Loss: 0.121971\n",
      "[intrinsic6_ambient12] Epoch 310 - Avg Loss: 0.122017\n",
      "[intrinsic6_ambient12] Epoch 320 - Avg Loss: 0.121883\n",
      "[intrinsic6_ambient12] Epoch 330 - Avg Loss: 0.121501\n",
      "[intrinsic6_ambient12] Epoch 340 - Avg Loss: 0.121506\n",
      "[intrinsic6_ambient12] Epoch 350 - Avg Loss: 0.121473\n",
      "[intrinsic6_ambient12] Epoch 360 - Avg Loss: 0.121838\n",
      "[intrinsic6_ambient12] Epoch 370 - Avg Loss: 0.121110\n",
      "[intrinsic6_ambient12] Epoch 380 - Avg Loss: 0.121125\n",
      "[intrinsic6_ambient12] Epoch 390 - Avg Loss: 0.120908\n",
      "[intrinsic6_ambient12] Epoch 400 - Avg Loss: 0.121034\n",
      "[intrinsic6_ambient12] Epoch 410 - Avg Loss: 0.120616\n",
      "[intrinsic6_ambient12] Epoch 420 - Avg Loss: 0.120048\n",
      "[intrinsic6_ambient12] Epoch 430 - Avg Loss: 0.120380\n",
      "[intrinsic6_ambient12] Epoch 440 - Avg Loss: 0.120986\n",
      "[intrinsic6_ambient12] Epoch 450 - Avg Loss: 0.120010\n",
      "[intrinsic6_ambient12] Epoch 460 - Avg Loss: 0.120747\n",
      "[intrinsic6_ambient12] Epoch 470 - Avg Loss: 0.121429\n",
      "[intrinsic6_ambient12] Epoch 480 - Avg Loss: 0.120551\n",
      "[intrinsic6_ambient12] Epoch 490 - Avg Loss: 0.119909\n",
      "[intrinsic6_ambient12] Epoch 500 - Avg Loss: 0.120755\n",
      "üîÅ Run 2/3\n",
      "[intrinsic6_ambient12] Epoch 1 - Avg Loss: 0.356311\n",
      "[intrinsic6_ambient12] Epoch 10 - Avg Loss: 0.170939\n",
      "[intrinsic6_ambient12] Epoch 20 - Avg Loss: 0.154048\n",
      "[intrinsic6_ambient12] Epoch 30 - Avg Loss: 0.145130\n",
      "[intrinsic6_ambient12] Epoch 40 - Avg Loss: 0.138825\n",
      "[intrinsic6_ambient12] Epoch 50 - Avg Loss: 0.134897\n",
      "[intrinsic6_ambient12] Epoch 60 - Avg Loss: 0.131897\n",
      "[intrinsic6_ambient12] Epoch 70 - Avg Loss: 0.130455\n",
      "[intrinsic6_ambient12] Epoch 80 - Avg Loss: 0.129913\n",
      "[intrinsic6_ambient12] Epoch 90 - Avg Loss: 0.126724\n",
      "[intrinsic6_ambient12] Epoch 100 - Avg Loss: 0.126869\n",
      "[intrinsic6_ambient12] Epoch 110 - Avg Loss: 0.125187\n",
      "[intrinsic6_ambient12] Epoch 120 - Avg Loss: 0.125465\n",
      "[intrinsic6_ambient12] Epoch 130 - Avg Loss: 0.124836\n",
      "[intrinsic6_ambient12] Epoch 140 - Avg Loss: 0.126411\n",
      "[intrinsic6_ambient12] Epoch 150 - Avg Loss: 0.124982\n",
      "[intrinsic6_ambient12] Epoch 160 - Avg Loss: 0.124613\n",
      "[intrinsic6_ambient12] Epoch 170 - Avg Loss: 0.123470\n",
      "[intrinsic6_ambient12] Epoch 180 - Avg Loss: 0.123415\n",
      "[intrinsic6_ambient12] Epoch 190 - Avg Loss: 0.123001\n",
      "[intrinsic6_ambient12] Epoch 200 - Avg Loss: 0.123472\n",
      "[intrinsic6_ambient12] Epoch 210 - Avg Loss: 0.122293\n",
      "[intrinsic6_ambient12] Epoch 220 - Avg Loss: 0.120813\n",
      "[intrinsic6_ambient12] Epoch 230 - Avg Loss: 0.121010\n",
      "[intrinsic6_ambient12] Epoch 240 - Avg Loss: 0.122527\n",
      "[intrinsic6_ambient12] Epoch 250 - Avg Loss: 0.122561\n",
      "[intrinsic6_ambient12] Epoch 260 - Avg Loss: 0.121942\n",
      "[intrinsic6_ambient12] Epoch 270 - Avg Loss: 0.121234\n",
      "[intrinsic6_ambient12] Epoch 280 - Avg Loss: 0.122759\n",
      "[intrinsic6_ambient12] Epoch 290 - Avg Loss: 0.120762\n",
      "[intrinsic6_ambient12] Epoch 300 - Avg Loss: 0.121607\n",
      "[intrinsic6_ambient12] Epoch 310 - Avg Loss: 0.121672\n",
      "[intrinsic6_ambient12] Epoch 320 - Avg Loss: 0.121577\n",
      "[intrinsic6_ambient12] Epoch 330 - Avg Loss: 0.122234\n",
      "[intrinsic6_ambient12] Epoch 340 - Avg Loss: 0.119750\n",
      "[intrinsic6_ambient12] Epoch 350 - Avg Loss: 0.122197\n",
      "[intrinsic6_ambient12] Epoch 360 - Avg Loss: 0.120062\n",
      "[intrinsic6_ambient12] Epoch 370 - Avg Loss: 0.120846\n",
      "[intrinsic6_ambient12] Epoch 380 - Avg Loss: 0.120697\n",
      "[intrinsic6_ambient12] Epoch 390 - Avg Loss: 0.121522\n",
      "[intrinsic6_ambient12] Epoch 400 - Avg Loss: 0.122071\n",
      "[intrinsic6_ambient12] Epoch 410 - Avg Loss: 0.120523\n",
      "[intrinsic6_ambient12] Epoch 420 - Avg Loss: 0.121227\n",
      "[intrinsic6_ambient12] Epoch 430 - Avg Loss: 0.121276\n",
      "[intrinsic6_ambient12] Epoch 440 - Avg Loss: 0.121097\n",
      "[intrinsic6_ambient12] Epoch 450 - Avg Loss: 0.120721\n",
      "[intrinsic6_ambient12] Epoch 460 - Avg Loss: 0.120834\n",
      "[intrinsic6_ambient12] Epoch 470 - Avg Loss: 0.120854\n",
      "[intrinsic6_ambient12] Epoch 480 - Avg Loss: 0.120813\n",
      "[intrinsic6_ambient12] Epoch 490 - Avg Loss: 0.121066\n",
      "[intrinsic6_ambient12] Epoch 500 - Avg Loss: 0.119935\n",
      "üîÅ Run 3/3\n",
      "[intrinsic6_ambient12] Epoch 1 - Avg Loss: 0.355470\n",
      "[intrinsic6_ambient12] Epoch 10 - Avg Loss: 0.170911\n",
      "[intrinsic6_ambient12] Epoch 20 - Avg Loss: 0.151817\n",
      "[intrinsic6_ambient12] Epoch 30 - Avg Loss: 0.143080\n",
      "[intrinsic6_ambient12] Epoch 40 - Avg Loss: 0.136272\n",
      "[intrinsic6_ambient12] Epoch 50 - Avg Loss: 0.134177\n",
      "[intrinsic6_ambient12] Epoch 60 - Avg Loss: 0.130907\n",
      "[intrinsic6_ambient12] Epoch 70 - Avg Loss: 0.128540\n",
      "[intrinsic6_ambient12] Epoch 80 - Avg Loss: 0.127259\n",
      "[intrinsic6_ambient12] Epoch 90 - Avg Loss: 0.126792\n",
      "[intrinsic6_ambient12] Epoch 100 - Avg Loss: 0.125690\n",
      "[intrinsic6_ambient12] Epoch 110 - Avg Loss: 0.126390\n",
      "[intrinsic6_ambient12] Epoch 120 - Avg Loss: 0.125506\n",
      "[intrinsic6_ambient12] Epoch 130 - Avg Loss: 0.125438\n",
      "[intrinsic6_ambient12] Epoch 140 - Avg Loss: 0.124445\n",
      "[intrinsic6_ambient12] Epoch 150 - Avg Loss: 0.123225\n",
      "[intrinsic6_ambient12] Epoch 160 - Avg Loss: 0.124835\n",
      "[intrinsic6_ambient12] Epoch 170 - Avg Loss: 0.123498\n",
      "[intrinsic6_ambient12] Epoch 180 - Avg Loss: 0.122268\n",
      "[intrinsic6_ambient12] Epoch 190 - Avg Loss: 0.121438\n",
      "[intrinsic6_ambient12] Epoch 200 - Avg Loss: 0.123389\n",
      "[intrinsic6_ambient12] Epoch 210 - Avg Loss: 0.123279\n",
      "[intrinsic6_ambient12] Epoch 220 - Avg Loss: 0.122040\n",
      "[intrinsic6_ambient12] Epoch 230 - Avg Loss: 0.121688\n",
      "[intrinsic6_ambient12] Epoch 240 - Avg Loss: 0.121888\n",
      "[intrinsic6_ambient12] Epoch 250 - Avg Loss: 0.121775\n",
      "[intrinsic6_ambient12] Epoch 260 - Avg Loss: 0.120713\n",
      "[intrinsic6_ambient12] Epoch 270 - Avg Loss: 0.122067\n",
      "[intrinsic6_ambient12] Epoch 280 - Avg Loss: 0.121400\n",
      "[intrinsic6_ambient12] Epoch 290 - Avg Loss: 0.120474\n",
      "[intrinsic6_ambient12] Epoch 300 - Avg Loss: 0.121714\n",
      "[intrinsic6_ambient12] Epoch 310 - Avg Loss: 0.121442\n",
      "[intrinsic6_ambient12] Epoch 320 - Avg Loss: 0.119952\n",
      "[intrinsic6_ambient12] Epoch 330 - Avg Loss: 0.121001\n",
      "[intrinsic6_ambient12] Epoch 340 - Avg Loss: 0.121153\n",
      "[intrinsic6_ambient12] Epoch 350 - Avg Loss: 0.121903\n",
      "[intrinsic6_ambient12] Epoch 360 - Avg Loss: 0.121608\n",
      "[intrinsic6_ambient12] Epoch 370 - Avg Loss: 0.121306\n",
      "[intrinsic6_ambient12] Epoch 380 - Avg Loss: 0.121127\n",
      "[intrinsic6_ambient12] Epoch 390 - Avg Loss: 0.121194\n",
      "[intrinsic6_ambient12] Epoch 400 - Avg Loss: 0.121702\n",
      "[intrinsic6_ambient12] Epoch 410 - Avg Loss: 0.121126\n",
      "[intrinsic6_ambient12] Epoch 420 - Avg Loss: 0.120103\n",
      "[intrinsic6_ambient12] Epoch 430 - Avg Loss: 0.120788\n",
      "[intrinsic6_ambient12] Epoch 440 - Avg Loss: 0.121020\n",
      "[intrinsic6_ambient12] Epoch 450 - Avg Loss: 0.119740\n",
      "[intrinsic6_ambient12] Epoch 460 - Avg Loss: 0.120876\n",
      "[intrinsic6_ambient12] Epoch 470 - Avg Loss: 0.121107\n",
      "[intrinsic6_ambient12] Epoch 480 - Avg Loss: 0.120573\n",
      "[intrinsic6_ambient12] Epoch 490 - Avg Loss: 0.119838\n",
      "[intrinsic6_ambient12] Epoch 500 - Avg Loss: 0.119308\n",
      "[intrinsic6_ambient12] üîç EVCE (dim alignment): 0.000013\n",
      "[intrinsic6_ambient12] üìâ MNEE (density error):  0.075148\n",
      "\n",
      "üöÄ Training on config: intrinsic6_ambient20, shape = (20000, 20)\n",
      "üîÅ Run 1/3\n",
      "[intrinsic6_ambient20] Epoch 1 - Avg Loss: 0.373998\n",
      "[intrinsic6_ambient20] Epoch 10 - Avg Loss: 0.137580\n",
      "[intrinsic6_ambient20] Epoch 20 - Avg Loss: 0.118106\n",
      "[intrinsic6_ambient20] Epoch 30 - Avg Loss: 0.103598\n",
      "[intrinsic6_ambient20] Epoch 40 - Avg Loss: 0.096770\n",
      "[intrinsic6_ambient20] Epoch 50 - Avg Loss: 0.092477\n",
      "[intrinsic6_ambient20] Epoch 60 - Avg Loss: 0.088298\n",
      "[intrinsic6_ambient20] Epoch 70 - Avg Loss: 0.086370\n",
      "[intrinsic6_ambient20] Epoch 80 - Avg Loss: 0.085999\n",
      "[intrinsic6_ambient20] Epoch 90 - Avg Loss: 0.084478\n",
      "[intrinsic6_ambient20] Epoch 100 - Avg Loss: 0.083268\n",
      "[intrinsic6_ambient20] Epoch 110 - Avg Loss: 0.081054\n",
      "[intrinsic6_ambient20] Epoch 120 - Avg Loss: 0.080668\n",
      "[intrinsic6_ambient20] Epoch 130 - Avg Loss: 0.081649\n",
      "[intrinsic6_ambient20] Epoch 140 - Avg Loss: 0.080561\n",
      "[intrinsic6_ambient20] Epoch 150 - Avg Loss: 0.080821\n",
      "[intrinsic6_ambient20] Epoch 160 - Avg Loss: 0.079138\n",
      "[intrinsic6_ambient20] Epoch 170 - Avg Loss: 0.079035\n",
      "[intrinsic6_ambient20] Epoch 180 - Avg Loss: 0.078937\n",
      "[intrinsic6_ambient20] Epoch 190 - Avg Loss: 0.078318\n",
      "[intrinsic6_ambient20] Epoch 200 - Avg Loss: 0.078287\n",
      "[intrinsic6_ambient20] Epoch 210 - Avg Loss: 0.077985\n",
      "[intrinsic6_ambient20] Epoch 220 - Avg Loss: 0.077963\n",
      "[intrinsic6_ambient20] Epoch 230 - Avg Loss: 0.078213\n",
      "[intrinsic6_ambient20] Epoch 240 - Avg Loss: 0.078130\n",
      "[intrinsic6_ambient20] Epoch 250 - Avg Loss: 0.078065\n",
      "[intrinsic6_ambient20] Epoch 260 - Avg Loss: 0.077275\n",
      "[intrinsic6_ambient20] Epoch 270 - Avg Loss: 0.076640\n",
      "[intrinsic6_ambient20] Epoch 280 - Avg Loss: 0.077426\n",
      "[intrinsic6_ambient20] Epoch 290 - Avg Loss: 0.076856\n",
      "[intrinsic6_ambient20] Epoch 300 - Avg Loss: 0.077179\n",
      "[intrinsic6_ambient20] Epoch 310 - Avg Loss: 0.076980\n",
      "[intrinsic6_ambient20] Epoch 320 - Avg Loss: 0.076733\n",
      "[intrinsic6_ambient20] Epoch 330 - Avg Loss: 0.077155\n",
      "[intrinsic6_ambient20] Epoch 340 - Avg Loss: 0.076709\n",
      "[intrinsic6_ambient20] Epoch 350 - Avg Loss: 0.076672\n",
      "[intrinsic6_ambient20] Epoch 360 - Avg Loss: 0.077128\n",
      "[intrinsic6_ambient20] Epoch 370 - Avg Loss: 0.076473\n",
      "[intrinsic6_ambient20] Epoch 380 - Avg Loss: 0.076224\n",
      "[intrinsic6_ambient20] Epoch 390 - Avg Loss: 0.075879\n",
      "[intrinsic6_ambient20] Epoch 400 - Avg Loss: 0.075778\n",
      "[intrinsic6_ambient20] Epoch 410 - Avg Loss: 0.076088\n",
      "[intrinsic6_ambient20] Epoch 420 - Avg Loss: 0.076668\n",
      "[intrinsic6_ambient20] Epoch 430 - Avg Loss: 0.075951\n",
      "[intrinsic6_ambient20] Epoch 440 - Avg Loss: 0.075438\n",
      "[intrinsic6_ambient20] Epoch 450 - Avg Loss: 0.075114\n",
      "[intrinsic6_ambient20] Epoch 460 - Avg Loss: 0.075256\n",
      "[intrinsic6_ambient20] Epoch 470 - Avg Loss: 0.074900\n",
      "[intrinsic6_ambient20] Epoch 480 - Avg Loss: 0.075661\n",
      "[intrinsic6_ambient20] Epoch 490 - Avg Loss: 0.075977\n",
      "[intrinsic6_ambient20] Epoch 500 - Avg Loss: 0.076120\n",
      "üîÅ Run 2/3\n",
      "[intrinsic6_ambient20] Epoch 1 - Avg Loss: 0.377133\n",
      "[intrinsic6_ambient20] Epoch 10 - Avg Loss: 0.138227\n",
      "[intrinsic6_ambient20] Epoch 20 - Avg Loss: 0.114382\n",
      "[intrinsic6_ambient20] Epoch 30 - Avg Loss: 0.103194\n",
      "[intrinsic6_ambient20] Epoch 40 - Avg Loss: 0.095197\n",
      "[intrinsic6_ambient20] Epoch 50 - Avg Loss: 0.090694\n",
      "[intrinsic6_ambient20] Epoch 60 - Avg Loss: 0.088752\n",
      "[intrinsic6_ambient20] Epoch 70 - Avg Loss: 0.086054\n",
      "[intrinsic6_ambient20] Epoch 80 - Avg Loss: 0.085081\n",
      "[intrinsic6_ambient20] Epoch 90 - Avg Loss: 0.083501\n",
      "[intrinsic6_ambient20] Epoch 100 - Avg Loss: 0.083078\n",
      "[intrinsic6_ambient20] Epoch 110 - Avg Loss: 0.082457\n",
      "[intrinsic6_ambient20] Epoch 120 - Avg Loss: 0.081031\n",
      "[intrinsic6_ambient20] Epoch 130 - Avg Loss: 0.081292\n",
      "[intrinsic6_ambient20] Epoch 140 - Avg Loss: 0.080176\n",
      "[intrinsic6_ambient20] Epoch 150 - Avg Loss: 0.080802\n",
      "[intrinsic6_ambient20] Epoch 160 - Avg Loss: 0.079670\n",
      "[intrinsic6_ambient20] Epoch 170 - Avg Loss: 0.078768\n",
      "[intrinsic6_ambient20] Epoch 180 - Avg Loss: 0.078782\n",
      "[intrinsic6_ambient20] Epoch 190 - Avg Loss: 0.078419\n",
      "[intrinsic6_ambient20] Epoch 200 - Avg Loss: 0.078956\n",
      "[intrinsic6_ambient20] Epoch 210 - Avg Loss: 0.077850\n",
      "[intrinsic6_ambient20] Epoch 220 - Avg Loss: 0.078245\n",
      "[intrinsic6_ambient20] Epoch 230 - Avg Loss: 0.077771\n",
      "[intrinsic6_ambient20] Epoch 240 - Avg Loss: 0.078659\n",
      "[intrinsic6_ambient20] Epoch 250 - Avg Loss: 0.077585\n",
      "[intrinsic6_ambient20] Epoch 260 - Avg Loss: 0.077079\n",
      "[intrinsic6_ambient20] Epoch 270 - Avg Loss: 0.078373\n",
      "[intrinsic6_ambient20] Epoch 280 - Avg Loss: 0.077293\n",
      "[intrinsic6_ambient20] Epoch 290 - Avg Loss: 0.077805\n",
      "[intrinsic6_ambient20] Epoch 300 - Avg Loss: 0.076456\n",
      "[intrinsic6_ambient20] Epoch 310 - Avg Loss: 0.077147\n",
      "[intrinsic6_ambient20] Epoch 320 - Avg Loss: 0.076825\n",
      "[intrinsic6_ambient20] Epoch 330 - Avg Loss: 0.077699\n",
      "[intrinsic6_ambient20] Epoch 340 - Avg Loss: 0.076494\n",
      "[intrinsic6_ambient20] Epoch 350 - Avg Loss: 0.077179\n",
      "[intrinsic6_ambient20] Epoch 360 - Avg Loss: 0.077130\n",
      "[intrinsic6_ambient20] Epoch 370 - Avg Loss: 0.075770\n",
      "[intrinsic6_ambient20] Epoch 380 - Avg Loss: 0.076192\n",
      "[intrinsic6_ambient20] Epoch 390 - Avg Loss: 0.076237\n",
      "[intrinsic6_ambient20] Epoch 400 - Avg Loss: 0.076290\n",
      "[intrinsic6_ambient20] Epoch 410 - Avg Loss: 0.076150\n",
      "[intrinsic6_ambient20] Epoch 420 - Avg Loss: 0.076695\n",
      "[intrinsic6_ambient20] Epoch 430 - Avg Loss: 0.075870\n",
      "[intrinsic6_ambient20] Epoch 440 - Avg Loss: 0.075911\n",
      "[intrinsic6_ambient20] Epoch 450 - Avg Loss: 0.075729\n",
      "[intrinsic6_ambient20] Epoch 460 - Avg Loss: 0.075365\n",
      "[intrinsic6_ambient20] Epoch 470 - Avg Loss: 0.076158\n",
      "[intrinsic6_ambient20] Epoch 480 - Avg Loss: 0.075376\n",
      "[intrinsic6_ambient20] Epoch 490 - Avg Loss: 0.075746\n",
      "[intrinsic6_ambient20] Epoch 500 - Avg Loss: 0.075338\n",
      "üîÅ Run 3/3\n",
      "[intrinsic6_ambient20] Epoch 1 - Avg Loss: 0.376278\n",
      "[intrinsic6_ambient20] Epoch 10 - Avg Loss: 0.137617\n",
      "[intrinsic6_ambient20] Epoch 20 - Avg Loss: 0.116932\n",
      "[intrinsic6_ambient20] Epoch 30 - Avg Loss: 0.103392\n",
      "[intrinsic6_ambient20] Epoch 40 - Avg Loss: 0.097246\n",
      "[intrinsic6_ambient20] Epoch 50 - Avg Loss: 0.092669\n",
      "[intrinsic6_ambient20] Epoch 60 - Avg Loss: 0.089402\n",
      "[intrinsic6_ambient20] Epoch 70 - Avg Loss: 0.086473\n",
      "[intrinsic6_ambient20] Epoch 80 - Avg Loss: 0.084736\n",
      "[intrinsic6_ambient20] Epoch 90 - Avg Loss: 0.084135\n",
      "[intrinsic6_ambient20] Epoch 100 - Avg Loss: 0.083656\n",
      "[intrinsic6_ambient20] Epoch 110 - Avg Loss: 0.082528\n",
      "[intrinsic6_ambient20] Epoch 120 - Avg Loss: 0.081875\n",
      "[intrinsic6_ambient20] Epoch 130 - Avg Loss: 0.080362\n",
      "[intrinsic6_ambient20] Epoch 140 - Avg Loss: 0.080239\n",
      "[intrinsic6_ambient20] Epoch 150 - Avg Loss: 0.079602\n",
      "[intrinsic6_ambient20] Epoch 160 - Avg Loss: 0.080403\n",
      "[intrinsic6_ambient20] Epoch 170 - Avg Loss: 0.079532\n",
      "[intrinsic6_ambient20] Epoch 180 - Avg Loss: 0.079004\n",
      "[intrinsic6_ambient20] Epoch 190 - Avg Loss: 0.078328\n",
      "[intrinsic6_ambient20] Epoch 200 - Avg Loss: 0.077953\n",
      "[intrinsic6_ambient20] Epoch 210 - Avg Loss: 0.078983\n",
      "[intrinsic6_ambient20] Epoch 220 - Avg Loss: 0.077938\n",
      "[intrinsic6_ambient20] Epoch 230 - Avg Loss: 0.078435\n",
      "[intrinsic6_ambient20] Epoch 240 - Avg Loss: 0.078408\n",
      "[intrinsic6_ambient20] Epoch 250 - Avg Loss: 0.076983\n",
      "[intrinsic6_ambient20] Epoch 260 - Avg Loss: 0.077523\n",
      "[intrinsic6_ambient20] Epoch 270 - Avg Loss: 0.076655\n",
      "[intrinsic6_ambient20] Epoch 280 - Avg Loss: 0.076997\n",
      "[intrinsic6_ambient20] Epoch 290 - Avg Loss: 0.077081\n",
      "[intrinsic6_ambient20] Epoch 300 - Avg Loss: 0.076349\n",
      "[intrinsic6_ambient20] Epoch 310 - Avg Loss: 0.077216\n",
      "[intrinsic6_ambient20] Epoch 320 - Avg Loss: 0.077591\n",
      "[intrinsic6_ambient20] Epoch 330 - Avg Loss: 0.076501\n",
      "[intrinsic6_ambient20] Epoch 340 - Avg Loss: 0.076174\n",
      "[intrinsic6_ambient20] Epoch 350 - Avg Loss: 0.076476\n",
      "[intrinsic6_ambient20] Epoch 360 - Avg Loss: 0.075223\n",
      "[intrinsic6_ambient20] Epoch 370 - Avg Loss: 0.075924\n",
      "[intrinsic6_ambient20] Epoch 380 - Avg Loss: 0.076377\n",
      "[intrinsic6_ambient20] Epoch 390 - Avg Loss: 0.076361\n",
      "[intrinsic6_ambient20] Epoch 400 - Avg Loss: 0.075592\n",
      "[intrinsic6_ambient20] Epoch 410 - Avg Loss: 0.075993\n",
      "[intrinsic6_ambient20] Epoch 420 - Avg Loss: 0.075453\n",
      "[intrinsic6_ambient20] Epoch 430 - Avg Loss: 0.075760\n",
      "[intrinsic6_ambient20] Epoch 440 - Avg Loss: 0.075488\n",
      "[intrinsic6_ambient20] Epoch 450 - Avg Loss: 0.075844\n",
      "[intrinsic6_ambient20] Epoch 460 - Avg Loss: 0.075577\n",
      "[intrinsic6_ambient20] Epoch 470 - Avg Loss: 0.074086\n",
      "[intrinsic6_ambient20] Epoch 480 - Avg Loss: 0.075370\n",
      "[intrinsic6_ambient20] Epoch 490 - Avg Loss: 0.075077\n",
      "[intrinsic6_ambient20] Epoch 500 - Avg Loss: 0.075105\n",
      "[intrinsic6_ambient20] üîç EVCE (dim alignment): 0.000040\n",
      "[intrinsic6_ambient20] üìâ MNEE (density error):  0.328900\n",
      "\n",
      "üöÄ Training on config: intrinsic9_ambient12, shape = (20000, 12)\n",
      "üîÅ Run 1/3\n",
      "[intrinsic9_ambient12] Epoch 1 - Avg Loss: 0.382415\n",
      "[intrinsic9_ambient12] Epoch 10 - Avg Loss: 0.255979\n",
      "[intrinsic9_ambient12] Epoch 20 - Avg Loss: 0.239255\n",
      "[intrinsic9_ambient12] Epoch 30 - Avg Loss: 0.232259\n",
      "[intrinsic9_ambient12] Epoch 40 - Avg Loss: 0.230979\n",
      "[intrinsic9_ambient12] Epoch 50 - Avg Loss: 0.224559\n",
      "[intrinsic9_ambient12] Epoch 60 - Avg Loss: 0.222748\n",
      "[intrinsic9_ambient12] Epoch 70 - Avg Loss: 0.219895\n",
      "[intrinsic9_ambient12] Epoch 80 - Avg Loss: 0.219162\n",
      "[intrinsic9_ambient12] Epoch 90 - Avg Loss: 0.216501\n",
      "[intrinsic9_ambient12] Epoch 100 - Avg Loss: 0.217429\n",
      "[intrinsic9_ambient12] Epoch 110 - Avg Loss: 0.215592\n",
      "[intrinsic9_ambient12] Epoch 120 - Avg Loss: 0.216241\n",
      "[intrinsic9_ambient12] Epoch 130 - Avg Loss: 0.213882\n",
      "[intrinsic9_ambient12] Epoch 140 - Avg Loss: 0.216059\n",
      "[intrinsic9_ambient12] Epoch 150 - Avg Loss: 0.214522\n",
      "[intrinsic9_ambient12] Epoch 160 - Avg Loss: 0.213428\n",
      "[intrinsic9_ambient12] Epoch 170 - Avg Loss: 0.211640\n",
      "[intrinsic9_ambient12] Epoch 180 - Avg Loss: 0.212914\n",
      "[intrinsic9_ambient12] Epoch 190 - Avg Loss: 0.211198\n",
      "[intrinsic9_ambient12] Epoch 200 - Avg Loss: 0.211703\n",
      "[intrinsic9_ambient12] Epoch 210 - Avg Loss: 0.212941\n",
      "[intrinsic9_ambient12] Epoch 220 - Avg Loss: 0.211033\n",
      "[intrinsic9_ambient12] Epoch 230 - Avg Loss: 0.211508\n",
      "[intrinsic9_ambient12] Epoch 240 - Avg Loss: 0.211102\n",
      "[intrinsic9_ambient12] Epoch 250 - Avg Loss: 0.210686\n",
      "[intrinsic9_ambient12] Epoch 260 - Avg Loss: 0.212106\n",
      "[intrinsic9_ambient12] Epoch 270 - Avg Loss: 0.211311\n",
      "[intrinsic9_ambient12] Epoch 280 - Avg Loss: 0.208887\n",
      "[intrinsic9_ambient12] Epoch 290 - Avg Loss: 0.211909\n",
      "[intrinsic9_ambient12] Epoch 300 - Avg Loss: 0.210429\n",
      "[intrinsic9_ambient12] Epoch 310 - Avg Loss: 0.209877\n",
      "[intrinsic9_ambient12] Epoch 320 - Avg Loss: 0.210103\n",
      "[intrinsic9_ambient12] Epoch 330 - Avg Loss: 0.210126\n",
      "[intrinsic9_ambient12] Epoch 340 - Avg Loss: 0.210198\n",
      "[intrinsic9_ambient12] Epoch 350 - Avg Loss: 0.211066\n",
      "[intrinsic9_ambient12] Epoch 360 - Avg Loss: 0.208955\n",
      "[intrinsic9_ambient12] Epoch 370 - Avg Loss: 0.209542\n",
      "[intrinsic9_ambient12] Epoch 380 - Avg Loss: 0.211772\n",
      "[intrinsic9_ambient12] Epoch 390 - Avg Loss: 0.210100\n",
      "[intrinsic9_ambient12] Epoch 400 - Avg Loss: 0.208591\n",
      "[intrinsic9_ambient12] Epoch 410 - Avg Loss: 0.208854\n",
      "[intrinsic9_ambient12] Epoch 420 - Avg Loss: 0.210487\n",
      "[intrinsic9_ambient12] Epoch 430 - Avg Loss: 0.210082\n",
      "[intrinsic9_ambient12] Epoch 440 - Avg Loss: 0.210601\n",
      "[intrinsic9_ambient12] Epoch 450 - Avg Loss: 0.210150\n",
      "[intrinsic9_ambient12] Epoch 460 - Avg Loss: 0.209438\n",
      "[intrinsic9_ambient12] Epoch 470 - Avg Loss: 0.208279\n",
      "[intrinsic9_ambient12] Epoch 480 - Avg Loss: 0.209381\n",
      "[intrinsic9_ambient12] Epoch 490 - Avg Loss: 0.209818\n",
      "[intrinsic9_ambient12] Epoch 500 - Avg Loss: 0.210430\n",
      "üîÅ Run 2/3\n",
      "[intrinsic9_ambient12] Epoch 1 - Avg Loss: 0.383061\n",
      "[intrinsic9_ambient12] Epoch 10 - Avg Loss: 0.256813\n",
      "[intrinsic9_ambient12] Epoch 20 - Avg Loss: 0.239854\n",
      "[intrinsic9_ambient12] Epoch 30 - Avg Loss: 0.233545\n",
      "[intrinsic9_ambient12] Epoch 40 - Avg Loss: 0.229284\n",
      "[intrinsic9_ambient12] Epoch 50 - Avg Loss: 0.227691\n",
      "[intrinsic9_ambient12] Epoch 60 - Avg Loss: 0.223054\n",
      "[intrinsic9_ambient12] Epoch 70 - Avg Loss: 0.222337\n",
      "[intrinsic9_ambient12] Epoch 80 - Avg Loss: 0.218892\n",
      "[intrinsic9_ambient12] Epoch 90 - Avg Loss: 0.219069\n",
      "[intrinsic9_ambient12] Epoch 100 - Avg Loss: 0.218718\n",
      "[intrinsic9_ambient12] Epoch 110 - Avg Loss: 0.215392\n",
      "[intrinsic9_ambient12] Epoch 120 - Avg Loss: 0.214625\n",
      "[intrinsic9_ambient12] Epoch 130 - Avg Loss: 0.215261\n",
      "[intrinsic9_ambient12] Epoch 140 - Avg Loss: 0.212866\n",
      "[intrinsic9_ambient12] Epoch 150 - Avg Loss: 0.212569\n",
      "[intrinsic9_ambient12] Epoch 160 - Avg Loss: 0.213989\n",
      "[intrinsic9_ambient12] Epoch 170 - Avg Loss: 0.215416\n",
      "[intrinsic9_ambient12] Epoch 180 - Avg Loss: 0.212357\n",
      "[intrinsic9_ambient12] Epoch 190 - Avg Loss: 0.213091\n",
      "[intrinsic9_ambient12] Epoch 200 - Avg Loss: 0.212436\n",
      "[intrinsic9_ambient12] Epoch 210 - Avg Loss: 0.212046\n",
      "[intrinsic9_ambient12] Epoch 220 - Avg Loss: 0.212025\n",
      "[intrinsic9_ambient12] Epoch 230 - Avg Loss: 0.212225\n",
      "[intrinsic9_ambient12] Epoch 240 - Avg Loss: 0.211432\n",
      "[intrinsic9_ambient12] Epoch 250 - Avg Loss: 0.212134\n",
      "[intrinsic9_ambient12] Epoch 260 - Avg Loss: 0.211981\n",
      "[intrinsic9_ambient12] Epoch 270 - Avg Loss: 0.212911\n",
      "[intrinsic9_ambient12] Epoch 280 - Avg Loss: 0.211379\n",
      "[intrinsic9_ambient12] Epoch 290 - Avg Loss: 0.212702\n",
      "[intrinsic9_ambient12] Epoch 300 - Avg Loss: 0.209681\n",
      "[intrinsic9_ambient12] Epoch 310 - Avg Loss: 0.211687\n",
      "[intrinsic9_ambient12] Epoch 320 - Avg Loss: 0.210488\n",
      "[intrinsic9_ambient12] Epoch 330 - Avg Loss: 0.211228\n",
      "[intrinsic9_ambient12] Epoch 340 - Avg Loss: 0.209616\n",
      "[intrinsic9_ambient12] Epoch 350 - Avg Loss: 0.210778\n",
      "[intrinsic9_ambient12] Epoch 360 - Avg Loss: 0.209330\n",
      "[intrinsic9_ambient12] Epoch 370 - Avg Loss: 0.210546\n",
      "[intrinsic9_ambient12] Epoch 380 - Avg Loss: 0.209790\n",
      "[intrinsic9_ambient12] Epoch 390 - Avg Loss: 0.209777\n",
      "[intrinsic9_ambient12] Epoch 400 - Avg Loss: 0.210080\n",
      "[intrinsic9_ambient12] Epoch 410 - Avg Loss: 0.209093\n",
      "[intrinsic9_ambient12] Epoch 420 - Avg Loss: 0.210206\n",
      "[intrinsic9_ambient12] Epoch 430 - Avg Loss: 0.208353\n",
      "[intrinsic9_ambient12] Epoch 440 - Avg Loss: 0.209702\n",
      "[intrinsic9_ambient12] Epoch 450 - Avg Loss: 0.208240\n",
      "[intrinsic9_ambient12] Epoch 460 - Avg Loss: 0.209999\n",
      "[intrinsic9_ambient12] Epoch 470 - Avg Loss: 0.208359\n",
      "[intrinsic9_ambient12] Epoch 480 - Avg Loss: 0.209812\n",
      "[intrinsic9_ambient12] Epoch 490 - Avg Loss: 0.210659\n",
      "[intrinsic9_ambient12] Epoch 500 - Avg Loss: 0.208101\n",
      "üîÅ Run 3/3\n",
      "[intrinsic9_ambient12] Epoch 1 - Avg Loss: 0.379687\n",
      "[intrinsic9_ambient12] Epoch 10 - Avg Loss: 0.257506\n",
      "[intrinsic9_ambient12] Epoch 20 - Avg Loss: 0.240584\n",
      "[intrinsic9_ambient12] Epoch 30 - Avg Loss: 0.231828\n",
      "[intrinsic9_ambient12] Epoch 40 - Avg Loss: 0.229705\n",
      "[intrinsic9_ambient12] Epoch 50 - Avg Loss: 0.224494\n",
      "[intrinsic9_ambient12] Epoch 60 - Avg Loss: 0.222892\n",
      "[intrinsic9_ambient12] Epoch 70 - Avg Loss: 0.222228\n",
      "[intrinsic9_ambient12] Epoch 80 - Avg Loss: 0.218604\n",
      "[intrinsic9_ambient12] Epoch 90 - Avg Loss: 0.217593\n",
      "[intrinsic9_ambient12] Epoch 100 - Avg Loss: 0.218072\n",
      "[intrinsic9_ambient12] Epoch 110 - Avg Loss: 0.215838\n",
      "[intrinsic9_ambient12] Epoch 120 - Avg Loss: 0.217372\n",
      "[intrinsic9_ambient12] Epoch 130 - Avg Loss: 0.214777\n",
      "[intrinsic9_ambient12] Epoch 140 - Avg Loss: 0.214710\n",
      "[intrinsic9_ambient12] Epoch 150 - Avg Loss: 0.214168\n",
      "[intrinsic9_ambient12] Epoch 160 - Avg Loss: 0.213274\n",
      "[intrinsic9_ambient12] Epoch 170 - Avg Loss: 0.213375\n",
      "[intrinsic9_ambient12] Epoch 180 - Avg Loss: 0.213746\n",
      "[intrinsic9_ambient12] Epoch 190 - Avg Loss: 0.213920\n",
      "[intrinsic9_ambient12] Epoch 200 - Avg Loss: 0.212876\n",
      "[intrinsic9_ambient12] Epoch 210 - Avg Loss: 0.211979\n",
      "[intrinsic9_ambient12] Epoch 220 - Avg Loss: 0.212227\n",
      "[intrinsic9_ambient12] Epoch 230 - Avg Loss: 0.214418\n",
      "[intrinsic9_ambient12] Epoch 240 - Avg Loss: 0.211602\n",
      "[intrinsic9_ambient12] Epoch 250 - Avg Loss: 0.211295\n",
      "[intrinsic9_ambient12] Epoch 260 - Avg Loss: 0.210431\n",
      "[intrinsic9_ambient12] Epoch 270 - Avg Loss: 0.211970\n",
      "[intrinsic9_ambient12] Epoch 280 - Avg Loss: 0.210196\n",
      "[intrinsic9_ambient12] Epoch 290 - Avg Loss: 0.209761\n",
      "[intrinsic9_ambient12] Epoch 300 - Avg Loss: 0.209589\n",
      "[intrinsic9_ambient12] Epoch 310 - Avg Loss: 0.209575\n",
      "[intrinsic9_ambient12] Epoch 320 - Avg Loss: 0.210534\n",
      "[intrinsic9_ambient12] Epoch 330 - Avg Loss: 0.211474\n",
      "[intrinsic9_ambient12] Epoch 340 - Avg Loss: 0.210930\n",
      "[intrinsic9_ambient12] Epoch 350 - Avg Loss: 0.209591\n",
      "[intrinsic9_ambient12] Epoch 360 - Avg Loss: 0.209018\n",
      "[intrinsic9_ambient12] Epoch 370 - Avg Loss: 0.210293\n",
      "[intrinsic9_ambient12] Epoch 380 - Avg Loss: 0.209276\n",
      "[intrinsic9_ambient12] Epoch 390 - Avg Loss: 0.209934\n",
      "[intrinsic9_ambient12] Epoch 400 - Avg Loss: 0.209753\n",
      "[intrinsic9_ambient12] Epoch 410 - Avg Loss: 0.209114\n",
      "[intrinsic9_ambient12] Epoch 420 - Avg Loss: 0.208823\n",
      "[intrinsic9_ambient12] Epoch 430 - Avg Loss: 0.210118\n",
      "[intrinsic9_ambient12] Epoch 440 - Avg Loss: 0.211288\n",
      "[intrinsic9_ambient12] Epoch 450 - Avg Loss: 0.210279\n",
      "[intrinsic9_ambient12] Epoch 460 - Avg Loss: 0.210280\n",
      "[intrinsic9_ambient12] Epoch 470 - Avg Loss: 0.208906\n",
      "[intrinsic9_ambient12] Epoch 480 - Avg Loss: 0.208523\n",
      "[intrinsic9_ambient12] Epoch 490 - Avg Loss: 0.208149\n",
      "[intrinsic9_ambient12] Epoch 500 - Avg Loss: 0.209460\n",
      "[intrinsic9_ambient12] üîç EVCE (dim alignment): 0.000007\n",
      "[intrinsic9_ambient12] üìâ MNEE (density error):  0.039037\n",
      "\n",
      "üöÄ Training on config: intrinsic9_ambient20, shape = (20000, 20)\n",
      "üîÅ Run 1/3\n",
      "[intrinsic9_ambient20] Epoch 1 - Avg Loss: 0.392428\n",
      "[intrinsic9_ambient20] Epoch 10 - Avg Loss: 0.195725\n",
      "[intrinsic9_ambient20] Epoch 20 - Avg Loss: 0.181280\n",
      "[intrinsic9_ambient20] Epoch 30 - Avg Loss: 0.166822\n",
      "[intrinsic9_ambient20] Epoch 40 - Avg Loss: 0.158898\n",
      "[intrinsic9_ambient20] Epoch 50 - Avg Loss: 0.152985\n",
      "[intrinsic9_ambient20] Epoch 60 - Avg Loss: 0.148070\n",
      "[intrinsic9_ambient20] Epoch 70 - Avg Loss: 0.143761\n",
      "[intrinsic9_ambient20] Epoch 80 - Avg Loss: 0.142861\n",
      "[intrinsic9_ambient20] Epoch 90 - Avg Loss: 0.140524\n",
      "[intrinsic9_ambient20] Epoch 100 - Avg Loss: 0.139680\n",
      "[intrinsic9_ambient20] Epoch 110 - Avg Loss: 0.137625\n",
      "[intrinsic9_ambient20] Epoch 120 - Avg Loss: 0.137918\n",
      "[intrinsic9_ambient20] Epoch 130 - Avg Loss: 0.136218\n",
      "[intrinsic9_ambient20] Epoch 140 - Avg Loss: 0.135594\n",
      "[intrinsic9_ambient20] Epoch 150 - Avg Loss: 0.134941\n",
      "[intrinsic9_ambient20] Epoch 160 - Avg Loss: 0.135388\n",
      "[intrinsic9_ambient20] Epoch 170 - Avg Loss: 0.133984\n",
      "[intrinsic9_ambient20] Epoch 180 - Avg Loss: 0.134406\n",
      "[intrinsic9_ambient20] Epoch 190 - Avg Loss: 0.132910\n",
      "[intrinsic9_ambient20] Epoch 200 - Avg Loss: 0.132881\n",
      "[intrinsic9_ambient20] Epoch 210 - Avg Loss: 0.132333\n",
      "[intrinsic9_ambient20] Epoch 220 - Avg Loss: 0.132672\n",
      "[intrinsic9_ambient20] Epoch 230 - Avg Loss: 0.132215\n",
      "[intrinsic9_ambient20] Epoch 240 - Avg Loss: 0.131143\n",
      "[intrinsic9_ambient20] Epoch 250 - Avg Loss: 0.131437\n",
      "[intrinsic9_ambient20] Epoch 260 - Avg Loss: 0.130670\n",
      "[intrinsic9_ambient20] Epoch 270 - Avg Loss: 0.130603\n",
      "[intrinsic9_ambient20] Epoch 280 - Avg Loss: 0.131021\n",
      "[intrinsic9_ambient20] Epoch 290 - Avg Loss: 0.131694\n",
      "[intrinsic9_ambient20] Epoch 300 - Avg Loss: 0.130297\n",
      "[intrinsic9_ambient20] Epoch 310 - Avg Loss: 0.130119\n",
      "[intrinsic9_ambient20] Epoch 320 - Avg Loss: 0.130299\n",
      "[intrinsic9_ambient20] Epoch 330 - Avg Loss: 0.130695\n",
      "[intrinsic9_ambient20] Epoch 340 - Avg Loss: 0.129948\n",
      "[intrinsic9_ambient20] Epoch 350 - Avg Loss: 0.128861\n",
      "[intrinsic9_ambient20] Epoch 360 - Avg Loss: 0.129595\n",
      "[intrinsic9_ambient20] Epoch 370 - Avg Loss: 0.129805\n",
      "[intrinsic9_ambient20] Epoch 380 - Avg Loss: 0.129379\n",
      "[intrinsic9_ambient20] Epoch 390 - Avg Loss: 0.130146\n",
      "[intrinsic9_ambient20] Epoch 400 - Avg Loss: 0.129524\n",
      "[intrinsic9_ambient20] Epoch 410 - Avg Loss: 0.129299\n",
      "[intrinsic9_ambient20] Epoch 420 - Avg Loss: 0.128540\n",
      "[intrinsic9_ambient20] Epoch 430 - Avg Loss: 0.128703\n",
      "[intrinsic9_ambient20] Epoch 440 - Avg Loss: 0.127304\n",
      "[intrinsic9_ambient20] Epoch 450 - Avg Loss: 0.128875\n",
      "[intrinsic9_ambient20] Epoch 460 - Avg Loss: 0.129424\n",
      "[intrinsic9_ambient20] Epoch 470 - Avg Loss: 0.127602\n",
      "[intrinsic9_ambient20] Epoch 480 - Avg Loss: 0.126966\n",
      "[intrinsic9_ambient20] Epoch 490 - Avg Loss: 0.128663\n",
      "[intrinsic9_ambient20] Epoch 500 - Avg Loss: 0.128919\n",
      "üîÅ Run 2/3\n",
      "[intrinsic9_ambient20] Epoch 1 - Avg Loss: 0.388117\n",
      "[intrinsic9_ambient20] Epoch 10 - Avg Loss: 0.196503\n",
      "[intrinsic9_ambient20] Epoch 20 - Avg Loss: 0.178559\n",
      "[intrinsic9_ambient20] Epoch 30 - Avg Loss: 0.167271\n",
      "[intrinsic9_ambient20] Epoch 40 - Avg Loss: 0.157736\n",
      "[intrinsic9_ambient20] Epoch 50 - Avg Loss: 0.152262\n",
      "[intrinsic9_ambient20] Epoch 60 - Avg Loss: 0.146703\n",
      "[intrinsic9_ambient20] Epoch 70 - Avg Loss: 0.142914\n",
      "[intrinsic9_ambient20] Epoch 80 - Avg Loss: 0.141736\n",
      "[intrinsic9_ambient20] Epoch 90 - Avg Loss: 0.139237\n",
      "[intrinsic9_ambient20] Epoch 100 - Avg Loss: 0.139309\n",
      "[intrinsic9_ambient20] Epoch 110 - Avg Loss: 0.138059\n",
      "[intrinsic9_ambient20] Epoch 120 - Avg Loss: 0.137172\n",
      "[intrinsic9_ambient20] Epoch 130 - Avg Loss: 0.135368\n",
      "[intrinsic9_ambient20] Epoch 140 - Avg Loss: 0.135125\n",
      "[intrinsic9_ambient20] Epoch 150 - Avg Loss: 0.133990\n",
      "[intrinsic9_ambient20] Epoch 160 - Avg Loss: 0.133586\n",
      "[intrinsic9_ambient20] Epoch 170 - Avg Loss: 0.133786\n",
      "[intrinsic9_ambient20] Epoch 180 - Avg Loss: 0.132680\n",
      "[intrinsic9_ambient20] Epoch 190 - Avg Loss: 0.133408\n",
      "[intrinsic9_ambient20] Epoch 200 - Avg Loss: 0.132544\n",
      "[intrinsic9_ambient20] Epoch 210 - Avg Loss: 0.132730\n",
      "[intrinsic9_ambient20] Epoch 220 - Avg Loss: 0.132002\n",
      "[intrinsic9_ambient20] Epoch 230 - Avg Loss: 0.131432\n",
      "[intrinsic9_ambient20] Epoch 240 - Avg Loss: 0.131048\n",
      "[intrinsic9_ambient20] Epoch 250 - Avg Loss: 0.131043\n",
      "[intrinsic9_ambient20] Epoch 260 - Avg Loss: 0.131133\n",
      "[intrinsic9_ambient20] Epoch 270 - Avg Loss: 0.130338\n",
      "[intrinsic9_ambient20] Epoch 280 - Avg Loss: 0.130265\n",
      "[intrinsic9_ambient20] Epoch 290 - Avg Loss: 0.130488\n",
      "[intrinsic9_ambient20] Epoch 300 - Avg Loss: 0.129950\n",
      "[intrinsic9_ambient20] Epoch 310 - Avg Loss: 0.130008\n",
      "[intrinsic9_ambient20] Epoch 320 - Avg Loss: 0.128257\n",
      "[intrinsic9_ambient20] Epoch 330 - Avg Loss: 0.129552\n",
      "[intrinsic9_ambient20] Epoch 340 - Avg Loss: 0.129486\n",
      "[intrinsic9_ambient20] Epoch 350 - Avg Loss: 0.129886\n",
      "[intrinsic9_ambient20] Epoch 360 - Avg Loss: 0.130070\n",
      "[intrinsic9_ambient20] Epoch 370 - Avg Loss: 0.127895\n",
      "[intrinsic9_ambient20] Epoch 380 - Avg Loss: 0.129196\n",
      "[intrinsic9_ambient20] Epoch 390 - Avg Loss: 0.129769\n",
      "[intrinsic9_ambient20] Epoch 400 - Avg Loss: 0.128279\n",
      "[intrinsic9_ambient20] Epoch 410 - Avg Loss: 0.128505\n",
      "[intrinsic9_ambient20] Epoch 420 - Avg Loss: 0.129086\n",
      "[intrinsic9_ambient20] Epoch 430 - Avg Loss: 0.128004\n",
      "[intrinsic9_ambient20] Epoch 440 - Avg Loss: 0.128169\n",
      "[intrinsic9_ambient20] Epoch 450 - Avg Loss: 0.128930\n",
      "[intrinsic9_ambient20] Epoch 460 - Avg Loss: 0.128209\n",
      "[intrinsic9_ambient20] Epoch 470 - Avg Loss: 0.129244\n",
      "[intrinsic9_ambient20] Epoch 480 - Avg Loss: 0.127328\n",
      "[intrinsic9_ambient20] Epoch 490 - Avg Loss: 0.128850\n",
      "[intrinsic9_ambient20] Epoch 500 - Avg Loss: 0.127546\n",
      "üîÅ Run 3/3\n",
      "[intrinsic9_ambient20] Epoch 1 - Avg Loss: 0.388111\n",
      "[intrinsic9_ambient20] Epoch 10 - Avg Loss: 0.195916\n",
      "[intrinsic9_ambient20] Epoch 20 - Avg Loss: 0.179480\n",
      "[intrinsic9_ambient20] Epoch 30 - Avg Loss: 0.168206\n",
      "[intrinsic9_ambient20] Epoch 40 - Avg Loss: 0.159565\n",
      "[intrinsic9_ambient20] Epoch 50 - Avg Loss: 0.154173\n",
      "[intrinsic9_ambient20] Epoch 60 - Avg Loss: 0.148900\n",
      "[intrinsic9_ambient20] Epoch 70 - Avg Loss: 0.144004\n",
      "[intrinsic9_ambient20] Epoch 80 - Avg Loss: 0.141192\n",
      "[intrinsic9_ambient20] Epoch 90 - Avg Loss: 0.141650\n",
      "[intrinsic9_ambient20] Epoch 100 - Avg Loss: 0.138542\n",
      "[intrinsic9_ambient20] Epoch 110 - Avg Loss: 0.137631\n",
      "[intrinsic9_ambient20] Epoch 120 - Avg Loss: 0.136527\n",
      "[intrinsic9_ambient20] Epoch 130 - Avg Loss: 0.136692\n",
      "[intrinsic9_ambient20] Epoch 140 - Avg Loss: 0.135013\n",
      "[intrinsic9_ambient20] Epoch 150 - Avg Loss: 0.134313\n",
      "[intrinsic9_ambient20] Epoch 160 - Avg Loss: 0.135123\n",
      "[intrinsic9_ambient20] Epoch 170 - Avg Loss: 0.133507\n",
      "[intrinsic9_ambient20] Epoch 180 - Avg Loss: 0.133596\n",
      "[intrinsic9_ambient20] Epoch 190 - Avg Loss: 0.132821\n",
      "[intrinsic9_ambient20] Epoch 200 - Avg Loss: 0.132726\n",
      "[intrinsic9_ambient20] Epoch 210 - Avg Loss: 0.130762\n",
      "[intrinsic9_ambient20] Epoch 220 - Avg Loss: 0.131951\n",
      "[intrinsic9_ambient20] Epoch 230 - Avg Loss: 0.132396\n",
      "[intrinsic9_ambient20] Epoch 240 - Avg Loss: 0.131685\n",
      "[intrinsic9_ambient20] Epoch 250 - Avg Loss: 0.131770\n",
      "[intrinsic9_ambient20] Epoch 260 - Avg Loss: 0.131588\n",
      "[intrinsic9_ambient20] Epoch 270 - Avg Loss: 0.131300\n",
      "[intrinsic9_ambient20] Epoch 280 - Avg Loss: 0.131674\n",
      "[intrinsic9_ambient20] Epoch 290 - Avg Loss: 0.130939\n",
      "[intrinsic9_ambient20] Epoch 300 - Avg Loss: 0.130620\n",
      "[intrinsic9_ambient20] Epoch 310 - Avg Loss: 0.130403\n",
      "[intrinsic9_ambient20] Epoch 320 - Avg Loss: 0.129696\n",
      "[intrinsic9_ambient20] Epoch 330 - Avg Loss: 0.129844\n",
      "[intrinsic9_ambient20] Epoch 340 - Avg Loss: 0.129006\n",
      "[intrinsic9_ambient20] Epoch 350 - Avg Loss: 0.129960\n",
      "[intrinsic9_ambient20] Epoch 360 - Avg Loss: 0.129584\n",
      "[intrinsic9_ambient20] Epoch 370 - Avg Loss: 0.128360\n",
      "[intrinsic9_ambient20] Epoch 380 - Avg Loss: 0.129086\n",
      "[intrinsic9_ambient20] Epoch 390 - Avg Loss: 0.129017\n",
      "[intrinsic9_ambient20] Epoch 400 - Avg Loss: 0.129093\n",
      "[intrinsic9_ambient20] Epoch 410 - Avg Loss: 0.129389\n",
      "[intrinsic9_ambient20] Epoch 420 - Avg Loss: 0.128902\n",
      "[intrinsic9_ambient20] Epoch 430 - Avg Loss: 0.129177\n",
      "[intrinsic9_ambient20] Epoch 440 - Avg Loss: 0.128683\n",
      "[intrinsic9_ambient20] Epoch 450 - Avg Loss: 0.128688\n",
      "[intrinsic9_ambient20] Epoch 460 - Avg Loss: 0.129230\n",
      "[intrinsic9_ambient20] Epoch 470 - Avg Loss: 0.128306\n",
      "[intrinsic9_ambient20] Epoch 480 - Avg Loss: 0.129363\n",
      "[intrinsic9_ambient20] Epoch 490 - Avg Loss: 0.128912\n",
      "[intrinsic9_ambient20] Epoch 500 - Avg Loss: 0.128557\n",
      "[intrinsic9_ambient20] üîç EVCE (dim alignment): 0.000017\n",
      "[intrinsic9_ambient20] üìâ MNEE (density error):  0.195809\n",
      "\n",
      "üöÄ Training on config: intrinsic12_ambient20, shape = (20000, 20)\n",
      "üîÅ Run 1/3\n",
      "[intrinsic12_ambient20] Epoch 1 - Avg Loss: 0.404902\n",
      "[intrinsic12_ambient20] Epoch 10 - Avg Loss: 0.247513\n",
      "[intrinsic12_ambient20] Epoch 20 - Avg Loss: 0.234674\n",
      "[intrinsic12_ambient20] Epoch 30 - Avg Loss: 0.223195\n",
      "[intrinsic12_ambient20] Epoch 40 - Avg Loss: 0.217286\n",
      "[intrinsic12_ambient20] Epoch 50 - Avg Loss: 0.212738\n",
      "[intrinsic12_ambient20] Epoch 60 - Avg Loss: 0.206977\n",
      "[intrinsic12_ambient20] Epoch 70 - Avg Loss: 0.206061\n",
      "[intrinsic12_ambient20] Epoch 80 - Avg Loss: 0.202118\n",
      "[intrinsic12_ambient20] Epoch 90 - Avg Loss: 0.199371\n",
      "[intrinsic12_ambient20] Epoch 100 - Avg Loss: 0.197208\n",
      "[intrinsic12_ambient20] Epoch 110 - Avg Loss: 0.195266\n",
      "[intrinsic12_ambient20] Epoch 120 - Avg Loss: 0.193936\n",
      "[intrinsic12_ambient20] Epoch 130 - Avg Loss: 0.193740\n",
      "[intrinsic12_ambient20] Epoch 140 - Avg Loss: 0.191493\n",
      "[intrinsic12_ambient20] Epoch 150 - Avg Loss: 0.191730\n",
      "[intrinsic12_ambient20] Epoch 160 - Avg Loss: 0.190048\n",
      "[intrinsic12_ambient20] Epoch 170 - Avg Loss: 0.191232\n",
      "[intrinsic12_ambient20] Epoch 180 - Avg Loss: 0.189898\n",
      "[intrinsic12_ambient20] Epoch 190 - Avg Loss: 0.188351\n",
      "[intrinsic12_ambient20] Epoch 200 - Avg Loss: 0.188373\n",
      "[intrinsic12_ambient20] Epoch 210 - Avg Loss: 0.188560\n",
      "[intrinsic12_ambient20] Epoch 220 - Avg Loss: 0.187384\n",
      "[intrinsic12_ambient20] Epoch 230 - Avg Loss: 0.186604\n",
      "[intrinsic12_ambient20] Epoch 240 - Avg Loss: 0.188401\n",
      "[intrinsic12_ambient20] Epoch 250 - Avg Loss: 0.186855\n",
      "[intrinsic12_ambient20] Epoch 260 - Avg Loss: 0.186720\n",
      "[intrinsic12_ambient20] Epoch 270 - Avg Loss: 0.186748\n",
      "[intrinsic12_ambient20] Epoch 280 - Avg Loss: 0.185694\n",
      "[intrinsic12_ambient20] Epoch 290 - Avg Loss: 0.185489\n",
      "[intrinsic12_ambient20] Epoch 300 - Avg Loss: 0.185902\n",
      "[intrinsic12_ambient20] Epoch 310 - Avg Loss: 0.185063\n",
      "[intrinsic12_ambient20] Epoch 320 - Avg Loss: 0.184256\n",
      "[intrinsic12_ambient20] Epoch 330 - Avg Loss: 0.184046\n",
      "[intrinsic12_ambient20] Epoch 340 - Avg Loss: 0.184804\n",
      "[intrinsic12_ambient20] Epoch 350 - Avg Loss: 0.184123\n",
      "[intrinsic12_ambient20] Epoch 360 - Avg Loss: 0.185834\n",
      "[intrinsic12_ambient20] Epoch 370 - Avg Loss: 0.184891\n",
      "[intrinsic12_ambient20] Epoch 380 - Avg Loss: 0.185387\n",
      "[intrinsic12_ambient20] Epoch 390 - Avg Loss: 0.184573\n",
      "[intrinsic12_ambient20] Epoch 400 - Avg Loss: 0.184734\n",
      "[intrinsic12_ambient20] Epoch 410 - Avg Loss: 0.185014\n",
      "[intrinsic12_ambient20] Epoch 420 - Avg Loss: 0.183490\n",
      "[intrinsic12_ambient20] Epoch 430 - Avg Loss: 0.183682\n",
      "[intrinsic12_ambient20] Epoch 440 - Avg Loss: 0.183289\n",
      "[intrinsic12_ambient20] Epoch 450 - Avg Loss: 0.182435\n",
      "[intrinsic12_ambient20] Epoch 460 - Avg Loss: 0.182805\n",
      "[intrinsic12_ambient20] Epoch 470 - Avg Loss: 0.182660\n",
      "[intrinsic12_ambient20] Epoch 480 - Avg Loss: 0.184450\n",
      "[intrinsic12_ambient20] Epoch 490 - Avg Loss: 0.182901\n",
      "[intrinsic12_ambient20] Epoch 500 - Avg Loss: 0.184331\n",
      "üîÅ Run 2/3\n",
      "[intrinsic12_ambient20] Epoch 1 - Avg Loss: 0.407945\n",
      "[intrinsic12_ambient20] Epoch 10 - Avg Loss: 0.247600\n",
      "[intrinsic12_ambient20] Epoch 20 - Avg Loss: 0.237437\n",
      "[intrinsic12_ambient20] Epoch 30 - Avg Loss: 0.224683\n",
      "[intrinsic12_ambient20] Epoch 40 - Avg Loss: 0.216550\n",
      "[intrinsic12_ambient20] Epoch 50 - Avg Loss: 0.212011\n",
      "[intrinsic12_ambient20] Epoch 60 - Avg Loss: 0.210393\n",
      "[intrinsic12_ambient20] Epoch 70 - Avg Loss: 0.205600\n",
      "[intrinsic12_ambient20] Epoch 80 - Avg Loss: 0.200994\n",
      "[intrinsic12_ambient20] Epoch 90 - Avg Loss: 0.199413\n",
      "[intrinsic12_ambient20] Epoch 100 - Avg Loss: 0.197562\n",
      "[intrinsic12_ambient20] Epoch 110 - Avg Loss: 0.194302\n",
      "[intrinsic12_ambient20] Epoch 120 - Avg Loss: 0.194060\n",
      "[intrinsic12_ambient20] Epoch 130 - Avg Loss: 0.191713\n",
      "[intrinsic12_ambient20] Epoch 140 - Avg Loss: 0.190979\n",
      "[intrinsic12_ambient20] Epoch 150 - Avg Loss: 0.191578\n",
      "[intrinsic12_ambient20] Epoch 160 - Avg Loss: 0.190090\n",
      "[intrinsic12_ambient20] Epoch 170 - Avg Loss: 0.191026\n",
      "[intrinsic12_ambient20] Epoch 180 - Avg Loss: 0.189118\n",
      "[intrinsic12_ambient20] Epoch 190 - Avg Loss: 0.188704\n",
      "[intrinsic12_ambient20] Epoch 200 - Avg Loss: 0.187715\n",
      "[intrinsic12_ambient20] Epoch 210 - Avg Loss: 0.187948\n",
      "[intrinsic12_ambient20] Epoch 220 - Avg Loss: 0.187593\n",
      "[intrinsic12_ambient20] Epoch 230 - Avg Loss: 0.186194\n",
      "[intrinsic12_ambient20] Epoch 240 - Avg Loss: 0.185981\n",
      "[intrinsic12_ambient20] Epoch 250 - Avg Loss: 0.185879\n",
      "[intrinsic12_ambient20] Epoch 260 - Avg Loss: 0.185525\n",
      "[intrinsic12_ambient20] Epoch 270 - Avg Loss: 0.186443\n",
      "[intrinsic12_ambient20] Epoch 280 - Avg Loss: 0.185666\n",
      "[intrinsic12_ambient20] Epoch 290 - Avg Loss: 0.186987\n",
      "[intrinsic12_ambient20] Epoch 300 - Avg Loss: 0.185566\n",
      "[intrinsic12_ambient20] Epoch 310 - Avg Loss: 0.185898\n",
      "[intrinsic12_ambient20] Epoch 320 - Avg Loss: 0.183526\n",
      "[intrinsic12_ambient20] Epoch 330 - Avg Loss: 0.185592\n",
      "[intrinsic12_ambient20] Epoch 340 - Avg Loss: 0.183653\n",
      "[intrinsic12_ambient20] Epoch 350 - Avg Loss: 0.185355\n",
      "[intrinsic12_ambient20] Epoch 360 - Avg Loss: 0.184725\n",
      "[intrinsic12_ambient20] Epoch 370 - Avg Loss: 0.183687\n",
      "[intrinsic12_ambient20] Epoch 380 - Avg Loss: 0.184099\n",
      "[intrinsic12_ambient20] Epoch 390 - Avg Loss: 0.184044\n",
      "[intrinsic12_ambient20] Epoch 400 - Avg Loss: 0.182388\n",
      "[intrinsic12_ambient20] Epoch 410 - Avg Loss: 0.183584\n",
      "[intrinsic12_ambient20] Epoch 420 - Avg Loss: 0.183880\n",
      "[intrinsic12_ambient20] Epoch 430 - Avg Loss: 0.182571\n",
      "[intrinsic12_ambient20] Epoch 440 - Avg Loss: 0.184333\n",
      "[intrinsic12_ambient20] Epoch 450 - Avg Loss: 0.182925\n",
      "[intrinsic12_ambient20] Epoch 460 - Avg Loss: 0.182854\n",
      "[intrinsic12_ambient20] Epoch 470 - Avg Loss: 0.182578\n",
      "[intrinsic12_ambient20] Epoch 480 - Avg Loss: 0.183807\n",
      "[intrinsic12_ambient20] Epoch 490 - Avg Loss: 0.181599\n",
      "[intrinsic12_ambient20] Epoch 500 - Avg Loss: 0.182711\n",
      "üîÅ Run 3/3\n",
      "[intrinsic12_ambient20] Epoch 1 - Avg Loss: 0.403285\n",
      "[intrinsic12_ambient20] Epoch 10 - Avg Loss: 0.246978\n",
      "[intrinsic12_ambient20] Epoch 20 - Avg Loss: 0.235277\n",
      "[intrinsic12_ambient20] Epoch 30 - Avg Loss: 0.223398\n",
      "[intrinsic12_ambient20] Epoch 40 - Avg Loss: 0.215465\n",
      "[intrinsic12_ambient20] Epoch 50 - Avg Loss: 0.211986\n",
      "[intrinsic12_ambient20] Epoch 60 - Avg Loss: 0.208476\n",
      "[intrinsic12_ambient20] Epoch 70 - Avg Loss: 0.203590\n",
      "[intrinsic12_ambient20] Epoch 80 - Avg Loss: 0.202843\n",
      "[intrinsic12_ambient20] Epoch 90 - Avg Loss: 0.198663\n",
      "[intrinsic12_ambient20] Epoch 100 - Avg Loss: 0.196780\n",
      "[intrinsic12_ambient20] Epoch 110 - Avg Loss: 0.195536\n",
      "[intrinsic12_ambient20] Epoch 120 - Avg Loss: 0.194437\n",
      "[intrinsic12_ambient20] Epoch 130 - Avg Loss: 0.192018\n",
      "[intrinsic12_ambient20] Epoch 140 - Avg Loss: 0.191158\n",
      "[intrinsic12_ambient20] Epoch 150 - Avg Loss: 0.191749\n",
      "[intrinsic12_ambient20] Epoch 160 - Avg Loss: 0.189524\n",
      "[intrinsic12_ambient20] Epoch 170 - Avg Loss: 0.189073\n",
      "[intrinsic12_ambient20] Epoch 180 - Avg Loss: 0.188873\n",
      "[intrinsic12_ambient20] Epoch 190 - Avg Loss: 0.188469\n",
      "[intrinsic12_ambient20] Epoch 200 - Avg Loss: 0.188745\n",
      "[intrinsic12_ambient20] Epoch 210 - Avg Loss: 0.188624\n",
      "[intrinsic12_ambient20] Epoch 220 - Avg Loss: 0.187256\n",
      "[intrinsic12_ambient20] Epoch 230 - Avg Loss: 0.187710\n",
      "[intrinsic12_ambient20] Epoch 240 - Avg Loss: 0.186604\n",
      "[intrinsic12_ambient20] Epoch 250 - Avg Loss: 0.186756\n",
      "[intrinsic12_ambient20] Epoch 260 - Avg Loss: 0.185837\n",
      "[intrinsic12_ambient20] Epoch 270 - Avg Loss: 0.184276\n",
      "[intrinsic12_ambient20] Epoch 280 - Avg Loss: 0.184773\n",
      "[intrinsic12_ambient20] Epoch 290 - Avg Loss: 0.185591\n",
      "[intrinsic12_ambient20] Epoch 300 - Avg Loss: 0.184887\n",
      "[intrinsic12_ambient20] Epoch 310 - Avg Loss: 0.186060\n",
      "[intrinsic12_ambient20] Epoch 320 - Avg Loss: 0.186074\n",
      "[intrinsic12_ambient20] Epoch 330 - Avg Loss: 0.183948\n",
      "[intrinsic12_ambient20] Epoch 340 - Avg Loss: 0.185912\n",
      "[intrinsic12_ambient20] Epoch 350 - Avg Loss: 0.184879\n",
      "[intrinsic12_ambient20] Epoch 360 - Avg Loss: 0.184448\n",
      "[intrinsic12_ambient20] Epoch 370 - Avg Loss: 0.183295\n",
      "[intrinsic12_ambient20] Epoch 380 - Avg Loss: 0.183251\n",
      "[intrinsic12_ambient20] Epoch 390 - Avg Loss: 0.183810\n",
      "[intrinsic12_ambient20] Epoch 400 - Avg Loss: 0.184245\n",
      "[intrinsic12_ambient20] Epoch 410 - Avg Loss: 0.183185\n",
      "[intrinsic12_ambient20] Epoch 420 - Avg Loss: 0.183674\n",
      "[intrinsic12_ambient20] Epoch 430 - Avg Loss: 0.183268\n",
      "[intrinsic12_ambient20] Epoch 440 - Avg Loss: 0.183068\n",
      "[intrinsic12_ambient20] Epoch 450 - Avg Loss: 0.184035\n",
      "[intrinsic12_ambient20] Epoch 460 - Avg Loss: 0.182918\n",
      "[intrinsic12_ambient20] Epoch 470 - Avg Loss: 0.183557\n",
      "[intrinsic12_ambient20] Epoch 480 - Avg Loss: 0.182598\n",
      "[intrinsic12_ambient20] Epoch 490 - Avg Loss: 0.182923\n",
      "[intrinsic12_ambient20] Epoch 500 - Avg Loss: 0.182662\n",
      "[intrinsic12_ambient20] üîç EVCE (dim alignment): 0.000005\n",
      "[intrinsic12_ambient20] üìâ MNEE (density error):  1.093580\n",
      "\n",
      "üìä Final Aggregated Results:\n",
      "intrinsic3_ambient12 ‚Üí EVCE: 0.0000, MNEE: 0.0831\n",
      "intrinsic3_ambient20 ‚Üí EVCE: 0.0001, MNEE: 0.2136\n",
      "intrinsic6_ambient12 ‚Üí EVCE: 0.0000, MNEE: 0.0751\n",
      "intrinsic6_ambient20 ‚Üí EVCE: 0.0000, MNEE: 0.3289\n",
      "intrinsic9_ambient12 ‚Üí EVCE: 0.0000, MNEE: 0.0390\n",
      "intrinsic9_ambient20 ‚Üí EVCE: 0.0000, MNEE: 0.1958\n",
      "intrinsic12_ambient20 ‚Üí EVCE: 0.0000, MNEE: 1.0936\n"
     ]
    }
   ],
   "source": [
    "train_diffusion_and_evaluate(all_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2272c6e7",
   "metadata": {},
   "source": [
    "Final Aggregated Results:\n",
    "\n",
    "intrinsic3_ambient12 ‚Üí EVCE: 0.0000, MNEE: 0.0831  \n",
    "\n",
    "intrinsic3_ambient20 ‚Üí EVCE: 0.0001, MNEE: 0.2136  \n",
    "\n",
    "intrinsic6_ambient12 ‚Üí EVCE: 0.0000, MNEE: 0.0751  \n",
    "\n",
    "intrinsic6_ambient20 ‚Üí EVCE: 0.0000, MNEE: 0.3289\n",
    "\n",
    "intrinsic9_ambient12 ‚Üí EVCE: 0.0000, MNEE: 0.0390\n",
    "\n",
    "intrinsic9_ambient20 ‚Üí EVCE: 0.0000, MNEE: 0.1958\n",
    "\n",
    "intrinsic12_ambient20 ‚Üí EVCE: 0.0000, MNEE: 1.0936"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d3adf",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "419f2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def pca_eigenvalue_error(real, fake, n_components=None):\n",
    "    pca_real = PCA(n_components=n_components)\n",
    "    pca_fake = PCA(n_components=n_components)\n",
    "\n",
    "    pca_real.fit(real)\n",
    "    pca_fake.fit(fake)\n",
    "\n",
    "    ev_real = pca_real.explained_variance_\n",
    "    ev_fake = pca_fake.explained_variance_\n",
    "\n",
    "    ev_real /= ev_real.sum()\n",
    "    ev_fake /= ev_fake.sum()\n",
    "\n",
    "    return np.mean(np.abs(ev_real - ev_fake))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb18be",
   "metadata": {},
   "source": [
    "## Sigmoid case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "16cb5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_sigmoid_datasets(seed=42, num_samples=20000):\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    all_datasets_sigmoid = {}\n",
    "\n",
    "    configs = [\n",
    "        (3, 7),\n",
    "        (3, 17),\n",
    "        (5, 11),\n",
    "        (5, 22),\n",
    "        (7, 15),\n",
    "        (7, 28),\n",
    "    ]\n",
    "\n",
    "    for intrinsic, ambient in configs:\n",
    "        padding_dim = ambient - (intrinsic + 1)  # +1 ÊòØ sigmoid Êâ©Â±ïÂá∫ÁöÑÁª¥Â∫¶\n",
    "        key = f\"intrinsic{intrinsic}_ambient{ambient}\"\n",
    "        dataset = SigmoidDataset(seed=seed, dimension=intrinsic, padding_dimension=padding_dim)\n",
    "        data = np.array(dataset.get_batch(num_samples))  # shape = [num_samples, ambient]\n",
    "        all_datasets_sigmoid[key] = data\n",
    "\n",
    "    return all_datasets_sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b6013048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffusion_on_sigmoid(all_datasets, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    os.makedirs(\"generated_sigmoid_samples\", exist_ok=True)\n",
    "    results = {}\n",
    "\n",
    "    for key, X in all_datasets.items():\n",
    "        print(f\"\\nüöÄ Training on config: {key}, shape = {X.shape}\")\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        dataset = TensorDataset(X_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "        model = DenoiseMLP(input_dim=X.shape[1]).to(device)\n",
    "        diffusion = Diffusion(model, timesteps=1000)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "        # === Training ===\n",
    "        for epoch in range(300):  # ÂèØË∞ÉÂ§ß\n",
    "            for batch in dataloader:\n",
    "                x0 = batch[0].to(device)\n",
    "                t = torch.randint(0, 1000, (x0.shape[0],), device=device)\n",
    "                loss = diffusion.p_losses(x0, t)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"[{key}] Epoch {epoch+1} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # === Sampling ===\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            samples = diffusion.sample(num_samples=20000, dim=X.shape[1], device=device)\n",
    "            samples = samples.cpu().numpy()\n",
    "            np.save(f\"generated_sigmoid_samples/{key}_samples.npy\", samples)\n",
    "\n",
    "        # === Evaluate ===\n",
    "        intrinsic = int(key.split(\"_\")[0].replace(\"intrinsic\", \"\"))\n",
    "        padding = int(key.split(\"_\")[1].replace(\"ambient\", \"\")) - (intrinsic + 1)\n",
    "        dataset_obj = SigmoidDataset(seed=0, dimension=intrinsic, padding_dimension=padding)\n",
    "\n",
    "        # 1Ô∏è‚É£ Manifold Error\n",
    "        scores = dataset_obj.score_batch(jnp.array(samples))\n",
    "        manifold_error = float(scores[\"Squared Norm of Manifold Dimension\"])\n",
    "        padding_error = float(scores[\"Squared Norm of Padding Dimensions\"])\n",
    "\n",
    "        # 2Ô∏è‚É£ PCA eigenvalue error\n",
    "        pca_error = pca_eigenvalue_error(X, samples, n_components=min(10, X.shape[1]))\n",
    "\n",
    "        results[key] = {\n",
    "            \"manifold_error\": manifold_error,\n",
    "            \"padding_error\": padding_error,\n",
    "            \"pca_error\": pca_error\n",
    "        }\n",
    "\n",
    "        print(f\"[{key}] ‚úÖ Manifold Error: {manifold_error:.6f} | Padding Error: {padding_error:.6f} | PCA Error: {pca_error:.6f}\")\n",
    "\n",
    "    print(\"\\nüìä Final Results:\")\n",
    "    for key, vals in results.items():\n",
    "        print(f\"{key} ‚Üí Manifold: {vals['manifold_error']:.4f}, Padding: {vals['padding_error']:.4f}, PCA: {vals['pca_error']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "75e76bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training on config: intrinsic3_ambient7, shape = (20000, 7)\n",
      "[intrinsic3_ambient7] Epoch 10 - Loss: 0.0876\n",
      "[intrinsic3_ambient7] Epoch 20 - Loss: 0.0888\n",
      "[intrinsic3_ambient7] Epoch 30 - Loss: 0.1440\n",
      "[intrinsic3_ambient7] Epoch 40 - Loss: 0.0885\n",
      "[intrinsic3_ambient7] Epoch 50 - Loss: 0.0837\n",
      "[intrinsic3_ambient7] Epoch 60 - Loss: 0.1153\n",
      "[intrinsic3_ambient7] Epoch 70 - Loss: 0.0998\n",
      "[intrinsic3_ambient7] Epoch 80 - Loss: 0.1214\n",
      "[intrinsic3_ambient7] Epoch 90 - Loss: 0.1166\n",
      "[intrinsic3_ambient7] Epoch 100 - Loss: 0.1025\n",
      "[intrinsic3_ambient7] Epoch 110 - Loss: 0.0900\n",
      "[intrinsic3_ambient7] Epoch 120 - Loss: 0.1030\n",
      "[intrinsic3_ambient7] Epoch 130 - Loss: 0.1072\n",
      "[intrinsic3_ambient7] Epoch 140 - Loss: 0.0964\n",
      "[intrinsic3_ambient7] Epoch 150 - Loss: 0.1116\n",
      "[intrinsic3_ambient7] Epoch 160 - Loss: 0.0779\n",
      "[intrinsic3_ambient7] Epoch 170 - Loss: 0.0850\n",
      "[intrinsic3_ambient7] Epoch 180 - Loss: 0.0676\n",
      "[intrinsic3_ambient7] Epoch 190 - Loss: 0.0671\n",
      "[intrinsic3_ambient7] Epoch 200 - Loss: 0.0820\n",
      "[intrinsic3_ambient7] Epoch 210 - Loss: 0.0946\n",
      "[intrinsic3_ambient7] Epoch 220 - Loss: 0.1192\n",
      "[intrinsic3_ambient7] Epoch 230 - Loss: 0.1032\n",
      "[intrinsic3_ambient7] Epoch 240 - Loss: 0.1341\n",
      "[intrinsic3_ambient7] Epoch 250 - Loss: 0.0940\n",
      "[intrinsic3_ambient7] Epoch 260 - Loss: 0.1026\n",
      "[intrinsic3_ambient7] Epoch 270 - Loss: 0.1049\n",
      "[intrinsic3_ambient7] Epoch 280 - Loss: 0.0828\n",
      "[intrinsic3_ambient7] Epoch 290 - Loss: 0.0850\n",
      "[intrinsic3_ambient7] Epoch 300 - Loss: 0.1352\n",
      "[intrinsic3_ambient7] ‚úÖ Manifold Error: 3.373352 | Padding Error: 0.000028 | PCA Error: 0.002233\n",
      "\n",
      "üöÄ Training on config: intrinsic3_ambient17, shape = (20000, 17)\n",
      "[intrinsic3_ambient17] Epoch 10 - Loss: 0.0529\n",
      "[intrinsic3_ambient17] Epoch 20 - Loss: 0.0398\n",
      "[intrinsic3_ambient17] Epoch 30 - Loss: 0.0562\n",
      "[intrinsic3_ambient17] Epoch 40 - Loss: 0.0467\n",
      "[intrinsic3_ambient17] Epoch 50 - Loss: 0.0515\n",
      "[intrinsic3_ambient17] Epoch 60 - Loss: 0.0524\n",
      "[intrinsic3_ambient17] Epoch 70 - Loss: 0.0400\n",
      "[intrinsic3_ambient17] Epoch 80 - Loss: 0.0541\n",
      "[intrinsic3_ambient17] Epoch 90 - Loss: 0.0377\n",
      "[intrinsic3_ambient17] Epoch 100 - Loss: 0.0322\n",
      "[intrinsic3_ambient17] Epoch 110 - Loss: 0.0473\n",
      "[intrinsic3_ambient17] Epoch 120 - Loss: 0.0349\n",
      "[intrinsic3_ambient17] Epoch 130 - Loss: 0.0510\n",
      "[intrinsic3_ambient17] Epoch 140 - Loss: 0.0369\n",
      "[intrinsic3_ambient17] Epoch 150 - Loss: 0.0469\n",
      "[intrinsic3_ambient17] Epoch 160 - Loss: 0.0485\n",
      "[intrinsic3_ambient17] Epoch 170 - Loss: 0.0343\n",
      "[intrinsic3_ambient17] Epoch 180 - Loss: 0.0351\n",
      "[intrinsic3_ambient17] Epoch 190 - Loss: 0.0364\n",
      "[intrinsic3_ambient17] Epoch 200 - Loss: 0.0537\n",
      "[intrinsic3_ambient17] Epoch 210 - Loss: 0.0447\n",
      "[intrinsic3_ambient17] Epoch 220 - Loss: 0.0475\n",
      "[intrinsic3_ambient17] Epoch 230 - Loss: 0.0442\n",
      "[intrinsic3_ambient17] Epoch 240 - Loss: 0.0492\n",
      "[intrinsic3_ambient17] Epoch 250 - Loss: 0.0388\n",
      "[intrinsic3_ambient17] Epoch 260 - Loss: 0.0579\n",
      "[intrinsic3_ambient17] Epoch 270 - Loss: 0.0465\n",
      "[intrinsic3_ambient17] Epoch 280 - Loss: 0.0499\n",
      "[intrinsic3_ambient17] Epoch 290 - Loss: 0.0614\n",
      "[intrinsic3_ambient17] Epoch 300 - Loss: 0.0467\n",
      "[intrinsic3_ambient17] ‚úÖ Manifold Error: 3.504953 | Padding Error: 0.000234 | PCA Error: 0.000621\n",
      "\n",
      "üöÄ Training on config: intrinsic5_ambient11, shape = (20000, 11)\n",
      "[intrinsic5_ambient11] Epoch 10 - Loss: 0.1014\n",
      "[intrinsic5_ambient11] Epoch 20 - Loss: 0.1071\n",
      "[intrinsic5_ambient11] Epoch 30 - Loss: 0.1170\n",
      "[intrinsic5_ambient11] Epoch 40 - Loss: 0.0850\n",
      "[intrinsic5_ambient11] Epoch 50 - Loss: 0.1138\n",
      "[intrinsic5_ambient11] Epoch 60 - Loss: 0.0998\n",
      "[intrinsic5_ambient11] Epoch 70 - Loss: 0.0861\n",
      "[intrinsic5_ambient11] Epoch 80 - Loss: 0.1195\n",
      "[intrinsic5_ambient11] Epoch 90 - Loss: 0.1122\n",
      "[intrinsic5_ambient11] Epoch 100 - Loss: 0.1032\n",
      "[intrinsic5_ambient11] Epoch 110 - Loss: 0.1098\n",
      "[intrinsic5_ambient11] Epoch 120 - Loss: 0.1131\n",
      "[intrinsic5_ambient11] Epoch 130 - Loss: 0.1499\n",
      "[intrinsic5_ambient11] Epoch 140 - Loss: 0.0907\n",
      "[intrinsic5_ambient11] Epoch 150 - Loss: 0.1102\n",
      "[intrinsic5_ambient11] Epoch 160 - Loss: 0.1049\n",
      "[intrinsic5_ambient11] Epoch 170 - Loss: 0.1139\n",
      "[intrinsic5_ambient11] Epoch 180 - Loss: 0.1018\n",
      "[intrinsic5_ambient11] Epoch 190 - Loss: 0.1360\n",
      "[intrinsic5_ambient11] Epoch 200 - Loss: 0.0998\n",
      "[intrinsic5_ambient11] Epoch 210 - Loss: 0.0908\n",
      "[intrinsic5_ambient11] Epoch 220 - Loss: 0.1101\n",
      "[intrinsic5_ambient11] Epoch 230 - Loss: 0.0842\n",
      "[intrinsic5_ambient11] Epoch 240 - Loss: 0.1302\n",
      "[intrinsic5_ambient11] Epoch 250 - Loss: 0.1125\n",
      "[intrinsic5_ambient11] Epoch 260 - Loss: 0.0970\n",
      "[intrinsic5_ambient11] Epoch 270 - Loss: 0.1166\n",
      "[intrinsic5_ambient11] Epoch 280 - Loss: 0.1035\n",
      "[intrinsic5_ambient11] Epoch 290 - Loss: 0.1256\n",
      "[intrinsic5_ambient11] Epoch 300 - Loss: 0.0997\n",
      "[intrinsic5_ambient11] ‚úÖ Manifold Error: 12.763991 | Padding Error: 0.000089 | PCA Error: 0.002341\n",
      "\n",
      "üöÄ Training on config: intrinsic5_ambient22, shape = (20000, 22)\n",
      "[intrinsic5_ambient22] Epoch 10 - Loss: 0.0596\n",
      "[intrinsic5_ambient22] Epoch 20 - Loss: 0.0622\n",
      "[intrinsic5_ambient22] Epoch 30 - Loss: 0.0627\n",
      "[intrinsic5_ambient22] Epoch 40 - Loss: 0.0641\n",
      "[intrinsic5_ambient22] Epoch 50 - Loss: 0.0646\n",
      "[intrinsic5_ambient22] Epoch 60 - Loss: 0.0649\n",
      "[intrinsic5_ambient22] Epoch 70 - Loss: 0.0582\n",
      "[intrinsic5_ambient22] Epoch 80 - Loss: 0.0636\n",
      "[intrinsic5_ambient22] Epoch 90 - Loss: 0.0762\n",
      "[intrinsic5_ambient22] Epoch 100 - Loss: 0.0540\n",
      "[intrinsic5_ambient22] Epoch 110 - Loss: 0.0530\n",
      "[intrinsic5_ambient22] Epoch 120 - Loss: 0.0542\n",
      "[intrinsic5_ambient22] Epoch 130 - Loss: 0.0568\n",
      "[intrinsic5_ambient22] Epoch 140 - Loss: 0.0371\n",
      "[intrinsic5_ambient22] Epoch 150 - Loss: 0.0535\n",
      "[intrinsic5_ambient22] Epoch 160 - Loss: 0.0624\n",
      "[intrinsic5_ambient22] Epoch 170 - Loss: 0.0618\n",
      "[intrinsic5_ambient22] Epoch 180 - Loss: 0.0726\n",
      "[intrinsic5_ambient22] Epoch 190 - Loss: 0.0485\n",
      "[intrinsic5_ambient22] Epoch 200 - Loss: 0.0710\n",
      "[intrinsic5_ambient22] Epoch 210 - Loss: 0.0497\n",
      "[intrinsic5_ambient22] Epoch 220 - Loss: 0.0648\n",
      "[intrinsic5_ambient22] Epoch 230 - Loss: 0.0744\n",
      "[intrinsic5_ambient22] Epoch 240 - Loss: 0.0629\n",
      "[intrinsic5_ambient22] Epoch 250 - Loss: 0.0689\n",
      "[intrinsic5_ambient22] Epoch 260 - Loss: 0.0607\n",
      "[intrinsic5_ambient22] Epoch 270 - Loss: 0.0437\n",
      "[intrinsic5_ambient22] Epoch 280 - Loss: 0.0596\n",
      "[intrinsic5_ambient22] Epoch 290 - Loss: 0.0675\n",
      "[intrinsic5_ambient22] Epoch 300 - Loss: 0.0549\n",
      "[intrinsic5_ambient22] ‚úÖ Manifold Error: 12.935655 | Padding Error: 0.000420 | PCA Error: 0.004076\n",
      "\n",
      "üöÄ Training on config: intrinsic7_ambient15, shape = (20000, 15)\n",
      "[intrinsic7_ambient15] Epoch 10 - Loss: 0.1265\n",
      "[intrinsic7_ambient15] Epoch 20 - Loss: 0.1210\n",
      "[intrinsic7_ambient15] Epoch 30 - Loss: 0.1071\n",
      "[intrinsic7_ambient15] Epoch 40 - Loss: 0.1190\n",
      "[intrinsic7_ambient15] Epoch 50 - Loss: 0.0963\n",
      "[intrinsic7_ambient15] Epoch 60 - Loss: 0.1261\n",
      "[intrinsic7_ambient15] Epoch 70 - Loss: 0.1325\n",
      "[intrinsic7_ambient15] Epoch 80 - Loss: 0.0968\n",
      "[intrinsic7_ambient15] Epoch 90 - Loss: 0.1343\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m all_sigmoid_datasets \u001b[38;5;241m=\u001b[39m generate_sigmoid_datasets()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_diffusion_on_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_sigmoid_datasets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[87], line 24\u001b[0m, in \u001b[0;36mtrain_diffusion_on_sigmoid\u001b[0;34m(all_datasets, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 24\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pycourse/lib/python3.8/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pycourse/lib/python3.8/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/pycourse/lib/python3.8/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/pycourse/lib/python3.8/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pycourse/lib/python3.8/site-packages/torch/optim/adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    390\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_sigmoid_datasets = generate_sigmoid_datasets()\n",
    "train_diffusion_on_sigmoid(all_sigmoid_datasets)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pycourse)",
   "language": "python",
   "name": "pycourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
